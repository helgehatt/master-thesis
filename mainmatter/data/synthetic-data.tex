\section{Synthetic Data}

Many communication networks such as social graphs usually comprise several groups of people who frequently interact. Hence, the synthetic data is generated by drawing two-dimensional initial positions for the nodes from multivariate Gaussians with known covariance. 
Each Gaussian simulates a cluster, where the nodes are connected depending on the distance between two nodes and the bias $\beta$. This is formally defined in \Cref{eq:synth-link} where $\sigma(x)$ is the sigmoid function (cf. eq. \ref{eq:lsm-proba}) and $\alpha$ is drawn from a uniform distribution to assign links with the probabilities given by the logistic regression.
\begin{equation}\label{eq:synth-link}
    \sub(y) = 
    \begin{cases}
        1 & \text{if } \sigma(\beta - \Vert\z_i - \z_j\Vert) \geq \alpha \qquad \alpha\sim U(0,1) \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}
The drawn initial positions form the latent space $\Z_0$ at the first time step. Latent space positions for subsequent time steps are generated by making small changes to the latent space. The changes depend entirely on which model the adjustment is supposed to mimic, where a static model assumes no change over time, while a diffusion model adds diffusion at each time step. Once latent space positions have been generated for all time steps, links are drawn according to the link probability, resulting in the target sociomatrix $\tY$. To keep the synthetic data simple, only undirected links are considered, as well as limiting the dataset to two dimensions ($k=2$) with 30 entities ($N=30$) over 100 time steps ($T=100$).

\subsection{Static Data}

    Initial positions are sampled from three multivariate Gaussians with means $(\pm2,2)$ as well as $(0,-2)$ and covariance $\bm{\Sigma}=0.5^2\bm{I}$, as illustrated in \Cref{Data/Static/Initial0}. 
    \xfig{\ximg[width=0.7\textwidth]{Data/Static/Initial0}[Latent space positions at time $t=0$]}
    The number of connected nodes depends on the chosen bias, which is used to skew the link probabilities in either direction. \Cref{Data/Static/BiasLow} illustrates the links using a bias of $\beta=-1$ while \Cref{Data/Static/BiasHigh} uses a bias of $\beta=1$, featuring 161 links opposed to 49 with negative bias.
    %The difference is 49 links opposed to 161 using the positive bias, including several links across clusters. 
    \xfig{\xdouble
        {\ximg{Data/Static/BiasLow}[Links using $\beta=-1$]}
        {\ximg{Data/Static/BiasHigh}[Links using $\beta=1$]}
    }
    Real-life networks are usually quite sparse, such that the negative bias of $\beta=-1$ is used unless otherwise stated, resulting in the number of links shown in \Cref{Data/Static/Links}. Even though the latent space positions remain constant over time, the positions are only used to find the probabilities of links occurring, such that the actual links at each time step are drawn from a uniform distribution according to the link probabilities.
    \xfig{\xdouble
        {\ximg{Data/Static/Links}[Links over time]}
        {\xanim{Data/Static/Data}[Static data]}
    }
    \Cref{Data/Static/Data} shows the synthetic static dataset for all 100 time steps (click on the image in Adobe Reader). The animation shows that the latent space positions do not change over time, while the links between nodes appear at random with higher frequency for nodes closer in latent space.

\subsection{Dynamic Data}

    The dynamic data is based on the same initial positions as the static data, however assumes that the latent positions for subsequent time steps are equal to the previous with diffusion added (eq. \ref{eq:dsnl-space}). This simulates the effect of observations drifting together and apart, as entities become more and less likely to form connections over time. The diffusion is sampled from a normal distribution with zero mean and $\bmsigma_\epsilon^2\bm{I}$ variance, known as the diffusion rate. \Cref{Data/Dynamic/DiffusionLow,Data/Dynamic/DiffusionHigh} illustrate how networks evolve over time with respect to diffusion, adding very little diffusion in the first and a lot in the second animation.
    \xfig{\xdouble
        {\xanim{Data/Dynamic/DiffusionLow}[Links adding diffusion with $\bmsigma_\epsilon=0.01\bm{I}$]}
        {\xanim{Data/Dynamic/DiffusionHigh}[Links adding diffusion with $\bmsigma_\epsilon=0.25\bm{I}$]}
    }
    The diffusion is not necessarily white noise, such that the dynamic data is generated by modifying the direction of the diffusion to simulate the three clusters dividing in half, forming three new clusters. The diffusion rate of $\bmsigma_\epsilon\bm{I}=0.05$ is then used to control the speed at which the transformation occurs, shown in \Cref{Data/Dynamic/Data}. 
    % making the observations change positions over time, while limiting the drift such that the positions are still possible to predict. 
    \xfig{\xdouble
        {\ximg{Data/Dynamic/Links}[Links over time]}
        {\xanim{Data/Dynamic/Data}[Dynamic data]}
    }
    As the amount of diffusion added in each step increases, the more uncertainty is revolved around the final positions of the observations, making them harder to accurately predict.

\subsection{Periodic Data}

    The autoregressive model is able to simulate periodic behavior by defining additional sets of initial positions and using appropriate values for $\bmphi$. Note that $\bmphi$ is a $p\times k$-dimensional matrix, while described as a vector of size $p$ when there is no distinction between any of the $k$ latent dimensions.
    The first set of initial positions is the same as the static and dynamic data, while the second and third sets of positions are defined using two Gaussians with means $(\pm2,0)$ and four Gaussians with means $(\pm2,\pm2)$ respectively and a variance of $0.5^2$, illustrated in Figures \ref{Data/Periodic/Initial1} and \ref{Data/Periodic/Initial2}.
    \xfig{\xdouble
        {\ximg{Data/Periodic/Initial1}[Initial positions at $t=1$]}
        {\ximg{Data/Periodic/Initial2}[Initial positions at $t=2$]}
    }
    Having predefined the initial positions for the first three time steps, a periodic pattern emerges using $\bmphi=[0, 0, 1]$, such that the latent positions at time $t$ is equal to the positions at time $t-3$, cf. \Cref{eq:ar-model}.
    Hence, the autoregressive model simulates an AR(3) process, including minimal diffusion using $\bmsigma_\epsilon=0.01$, emphasizing the periodicity of the dataset.
    
    The AR-coefficients, $\bmphi$, have a significant impact on the predictions of the model, where minor changes to the values is the difference between a stationary and non-stationary process. Depending on the underlying process, stationarity is not always desirable, however using too high or too low $\bmphi$s the predictions can quickly converge to infinity or zero over time. 
    To illustrate these two scenarios, \Cref{Data/Periodic/ARNone} shows the positions at the next time step using $\bmphi=[0,0,0]$ where the observations are separated according to the diffusion rate, being so low that all nodes are connected. 
    \xfig{\xdouble
        {\ximg{Data/Periodic/ARNone}[Positions at $t_3$ using $\bmphi=[0,0,0]$, resulting in white noise with $\sigma=0.01$.]}
        {\ximg{Data/Periodic/ARAll}[Positions at $t_3$ using $\bmphi=[1,1,1]$]}
    }
    The other scenario is depicted in \Cref{Data/Periodic/ARAll} using $\bmphi=[1,1,1]$, where the latent positions at the next time step is a sum of all three most recent sets of positions, resulting in too large distances to barely connect a few nodes.
    
    The generated periodic data has abrupt changes to the number of links over time, as can be expected with the large variations between the sets of initial positions. The changes do however follow a distinct pattern, as shown in \Cref{Data/Periodic/Links}, indicating the periodic behavior animated in \Cref{Data/Periodic/Data}.
    \xfig{\xdouble
        {\ximg{Data/Periodic/Links}[Number of links as a function of time]}
        {\xanim|3|{Data/Periodic/Data}[Animation of the periodic data]}
    }