
\chapter{Method}\label{ch:Method}

This chapter describes the methodology, introducing relevant approaches to latent space modeling and how these are adapted to solve temporal link prediction. The chapter also explains how the developed models are evaluated and compared, as well as how the models are trained and optimized.

\section{Model Development}

    Three different types of models are considered, the first one being a static latent space model. The latent space model forms the foundation for the dynamic models, where few additions are required to capture the temporal aspect with respect to diffusion. The diffusion model is one of the simplest dynamic models, which is extended to an autoregressive model capable of identifying periodic patterns in temporal data.

    \subsection{Latent Space Model}
    
        The latent space approach to social network analysis is introduced by \citeauthor{hoff2002latent} \cite{hoff2002latent}, using a model similar to Multidimensional Scaling in which entities are associated with locations in a $k$-dimensional space and where links are more likely if the entities are close in latent space.
        Given two entities $i$ and $j$, linkage is denoted by $\link$ and absence of a link by $\link*$, while $p(\link)$ or just $\sub(p)$ denotes the probability of observing the link. 
        Euclidean distance is used to measure the similarity between entities in the latent space, denoted as $\sub(d)$, however any distance metric satisfying the triangle inequality can be used.
        The latent space model is inherently reciprocal, where a small $\sub(d)$ follows from $\link$, making the link $\link(j)(i)$ more probable. Similarly, small $\sub(d)$ and $\sub(d)(jk)$ results in small $\sub(d)(ik)$, making the model inherently transitive as well.
        
        The method proposed in \cite{hoff2002latent} uses a conditional independence approach, assuming that the presence or absence of a link between two observations is independent of all other links in the network. Given the latent space positions $Z$, the covariate information $X$ and parameters $\theta$ the model is formalized in \Cref{eq:lsm-link-proba}. 
        
        \begin{equation}\label{eq:lsm-link-proba}
            P(Y|Z,X,\theta) = \prod_{i\neq j} P(\sub(y)|z_i,z_j,\sub(x),\theta)
        \end{equation}
        
        Covariate information includes pair-specific characteristics such as an indicator of whether two actors $i$ and $j$ are of the same sex. This was used by \citeauthor*{hoff2002latent} when analyzing strong friendship ties between boys and girls in a sixth-grade classroom. Providing such information can help increase model accuracy without adding much more complexity. However, the goal is to devise generalized models applicable to any type of network, such that covariate information is disregarded in the thesis. 
        
        Deciding whether links are present or absent can be considered a binary classification task, making the logistic regression model suitable to compute the odds of a link $\eta$ using the latent space positions $Z$ and a single parameter $\beta$ for the bias (intercept), as in \Cref{eq:lsm-link-odds}.
        
        \begin{equation}\label{eq:lsm-link-odds}
            \begin{split}
                \sub(\eta) &= \log \odds(\sub(y)=1|z_i,z_j,\beta) \\
                           &= \beta - |z_i-z_j|
            \end{split}
        \end{equation}
        
        To transform the odds to a probability measure, the sigmoid function is applied to $\eta$ in \Cref{eq:lsm-link-sigmoid}, restricting the output to the interval $[0,1]$.
        
        \begin{equation}\label{eq:lsm-link-sigmoid}
            \sub(p) = \sigma(\sub(\eta)) = \frac{1}{1 + e^{-(\beta - |z_i-z_j|)}}
        \end{equation}
        
        The link probabilities $\sub(p)$ is thus inverse proportional to the distance between the observations in the latent space, with a bias term to skew the probabilities in either direction.
        Having a conditional independence model, the log-likelihood is simply defined in \Cref{eq:lsm-log-likelihood}, which is used to estimate the model parameters using maximum likelihood techniques, or equivalently, by minimizing the negative log-likelihood function.
        
        \begin{equation}\label{eq:lsm-log-likelihood}
            \log P(Y|\eta) = \sum_{i\neq j}\{ \sub(\eta)\sub(y) - \log(1+e^{\sub(\eta)}) \}
        \end{equation}
        
        Note that the distances between a set of points in Euclidean space are invariant under rotation, reflection, and translation. Hence, there is an infinite number of latent space positions giving the same log-likelihood. The number of parameters to estimate depends on the number of entities $N$ in the network as well as the chosen dimensionality $k$ of the latent space, resulting in $N \times k + 1$ parameters. The $N\times k$ parameters corresponds to the latent space representation, while the $+1$ is due to the bias which is kept as a single scalar. \citeauthor{jacobsen2018a} experimented with specific biases for each row and column of the sociomatrix, corresponding to two vectors of dimensions $N\times 1$ and $1\times N$, though the results showed a single scalar to generalize better, allowing less wiggle room in the estimation of latent positions.
        
    \subsection{Diffusion Model}
        
        A dynamic latent space model for social network analysis is presented by \citeauthor{sarkar2005dynamic} in \cite{sarkar2005dynamic} using a standard Markov assumption, i.e. that the latent locations at time $t+1$ are conditionally independent of all previous locations given the latent locations at time $t$. The solution is comprised of two parts, an observation model and a transition model. As before, $\sub(d)=|z_i-z_j|$ is the Euclidean distance between observations in the latent space at a given time $t$.
        
        The observation model is similar to the latent space model, however with two important alterations. Firstly, each entity is associated with a radius $r_i$, allowing entities to vary their sociability by limiting their sphere of interaction within the latent space.
        Secondly, the link probability is weighed by a kernel function to ensure a high probability of linkage if latent positions are within $\sub(r)=max(r_i,r_j)$ of one another, and a constant noise probability $\epsilon$ otherwise.
        
        For optimization purposes, the kernelized function must be continuous and differentiable at $\sub(d)=\sub(r)$, hence choosing the biquadratic kernel as follows
        \begin{equation}\label{eq:biquadratic-kernel}
            K(\sub(d))=
            \begin{cases}
                (1-(\sub(d)/\sub(r))^2)^2 & \text{when } \sub(d) \leq \sub(r) \\
                \ 0 & \text{otherwise}
            \end{cases}
        \end{equation}
        resulting in the link probability shown in \Cref{eq:dsnl-link-proba}.
        
        \begin{equation}\label{eq:dsnl-link-proba}
            \sub(p)=\frac{1}{1 + e^{-(\sub(r)-\sub(d))}} K(\sub(d)) + \epsilon(1-K(\sub(d)))
        \end{equation}
        
        The second part, namely the transition model, penalizes large displacements from the previous time step by having each coordinate of each latent position independently subjected to a Gaussian perturbation with zero mean and $\sigma^2$ variance. This results in the following Gaussian model
        \begin{equation}\label{eq:dsnl-transition}
            \log P(Z_t|Z_{t-1}) = -\sum_{i=1}^n |Z_{i,t}-Z_{i,t-1}|^2 / \sigma^2 + c
        \end{equation}
        where the goal is to optimize log-likelihood of the graphs $Y_{0..t}$ conditioned on the latent positions $Z_{0..t}$.
        
        Using these elements to account for the temporal aspect, a simple diffusion model is formalized in \Cref{eq:dsnl-model} by applying a standard Markov assumption. 
        
        \begin{equation}\label{eq:dsnl-model}
            P(Y_t|Z_t,\beta,\sigma_\epsilon) = \prod_{\link}\sub(p)\prod_{\link*}(1-\sub(p))
        \end{equation}
        
        The link probabilities $\sub(p)$ are calculated as before, however with the latent space at a given time step modeled as the previous latent space with added diffusion, as shown in \Cref{eq:dsnl-latent-space}. The diffusion follows a Gaussian with zero mean and $\sigma_\epsilon^2$ variance, known as the diffusion rate.
        
        \begin{equation}\label{eq:dsnl-latent-space}
            Z_t = Z_{t-1} + \epsilon_t \qquad\qquad \epsilon \sim N(0,\sigma_\epsilon^2)
        \end{equation}
        
        The diffusion rate determines how far observations can drift in a single time step, such that a fairly low diffusion rate results in the observations staying more or less in place, while a high diffusion rate makes the observations drift apart, resulting in radical changes to the relationships. With the addition of the diffusion rate, the number of parameters to estimate in terms of the diffusion model is increased by one, to $n \times k + 2$. Note that as $t\rightarrow\infty$, we have $E[Z_t]=Z_0$ regardless of the magnitude of the diffusion rate as $E[\epsilon_t]=0$.
        
    
    \subsection{Autoregressive Model}
    
        Temporal autoregressive (AR) network models are discussed by \citeauthor{sewell2018simultaneous} in \cite{sewell2018simultaneous}, arguing that the assumption of conditional independence can be quite strong and difficult to justify. Their proposal is a general observation-driven model using a flexible autoregressive approach, extending the diffusion model by allowing predictions to be based on a number of past observations, thus moving away from the standard Markov assumption. 
        
        The autoregressive model considers the change in latent positions to be defined as in \Cref{eq:ar-model}, where $c$ is a constant, $p$ indicates the order of the AR process and the $\phi$s are the coefficients weighing the impact of the previous sets of latent positions.
        
        \begin{equation}\label{eq:ar-model}
            Z_t = c + \sum_{i=1}^p \phi_i Z_{t-i} + \epsilon_t \qquad\qquad \epsilon \sim N(0,\sigma_\epsilon^2)
        \end{equation}
        
        A first-order autoregressive process, denoted AR(1) as $p=1$, with $c=0$ and $\phi_1=1$ is identical to the previously described diffusion process, using only the most recent value in its prediction. Hence, the AR model is capable of capturing diffusion, while extending to periodic behavior as well for higher-order processes ($p>1$). An AR(2) process is able to model a signal which spikes every second time step, while an AR(3) process detects periodicity with an interval of 3, and so on. The order of the process is usually predetermined according to the expected periodic frequency of the data, often referred to as the seasonality. To illustrate, monthly data has a seasonal interval of twelve as there are twelve months in a year, while daily data has a seasonal interval of seven corresponding to a week.
        
        Increasing the order of the autoregressive model does however also increase model complexity, where an AR(3) process requires three sets of initial latent space positions to produce predictions for time steps $t>3$. Hence, the number of parameters to estimate has increased by a three-fold, in addition to multiple AR coefficients. For the general AR($p$) model, the number of parameters is $p\times (n\times k + 1) + 2$, requiring an additional set of latent space positions and an extra coefficient for each order of $p$.
        Using all AR coefficients is however not necessary and the basis for predictions can be limited to a fixed set of previous values, also called lags. Considering daily data, an AR(7) process is required to capture the expected weekly seasonality, where the fifth and sixth lags corresponding to the values at $t-5$ and $t-6$, often lack new information. Therefore, a sufficient limitation is to simply model the lags at 1, 2 and 7.
        
        An important concept of time series models such as the AR model is stationarity, where a stationary process is considered stable. For an AR(1) model, the condition $-1<\phi<1$ is necessary for the process to be stationary. This is proven in \cite{alonso2012autoregressive} by assuming that the process begins with an arbitrary fixed value $Z_0=x$, and writing up subsequent values by successive substitution.
        \begin{equation}
            \begin{split}
                Z_1 & = c + \phi x + \epsilon_1 \\
                Z_2 & = c(1+\phi) + \phi^2x+\phi\epsilon_1 + \epsilon_2 \\
                Z_3 & = c(1+\phi+\phi^2) + \phi^3x + \phi^2\epsilon_1 + \phi\epsilon_2 + \epsilon_3 \\
                & \vdotswithin{=}\qquad\qquad\qquad \vdots \\
                Z_t & = c\sum_{i=0}^{t-1}\phi^i + \phi^tx + \sum_{i=0}^{t-1}\phi^i\epsilon_{t-i}
            \end{split}
        \end{equation}
        The mean, or expected value, of $Z_t$ is shown in \Cref{eq:ar-expectation} where $E[\epsilon_t]=0$.
        \begin{equation}\label{eq:ar-expectation}
            E[Z_t] = c\sum_{i=0}^{t-1}\phi^i+\phi^tx
        \end{equation}
        For the process to be stationary, the mean must remain constant over time, hence both terms must converge to constants. This is the case if $|\phi|<1$, as the first term $\sum_{i=0}^{t-1}\phi^i$ becomes the sum of a geometric progression with ratio $\phi$ and converges to $c/(1-\phi)$, while the second term $\phi^t$ converges to zero. In this case, as $t\rightarrow\infty$, all the variables $Z_t$ have the same expectation $\mu=c/(1-\phi)$.
        
        Other stationarity conditions apply for autoregressive processes of higher order, which are calculated in \cite{giles2012stationarity} for an AR(2) model, resulting in the constraints forming the region in \Cref{eq:ar-2-constraints}.
        \begin{equation}\label{eq:ar-2-constraints}
            -1<\phi_2<1 \qquad \phi_1+\phi_2<1 \qquad \phi_2-\phi_1<1
        \end{equation}
        While these restrictions can be used to control the estimation of the AR parameters, it is not necessarily beneficial to limit the model to only capture stationary processes. Instead, by allowing the autoregressive model to estimate the coefficients freely, the parameters can be used to determine the stationarity of the underlying process.
        
        % Using the lag operator $B$, defined as $BZ_t=Z_{t-1}$, and letting $\tilde{Z}_t=Z_t-\mu$ the general AR(p) process is expressed as 
        % \begin{equation}\label{eq:ar-lag-model}
        %     (1-\phi_1B-...-\phi_pB^p)\tilde{Z}_t=\epsilon_t
        % \end{equation}
        
    
\section{Model Evaluation}
    
    The models are evaluated in terms of temporal link prediction, where link data for $T$ time steps is used to predict the links at time steps $T+1$, $T+2$ and so on. Hence, the first step is to split the dataset into a training and a test set, using roughly the first 80\% of the data for training, and the remainder for model evaluation. A dataset with $N$ entities consists of an $N\times N$ sociomatrix $Y$ at each time step, where $\sub(y)$ is 1 if entities $i$ and $j$ are linked at that time step, and 0 otherwise. As a result, the task boils down to a binary classification problem, predicting which entities are linked at which time steps.
    
    One metric for evaluating binary classifiers is accuracy, defined as the proportion of true results among the total number of cases examined, shown in \Cref{eq:eval-accuracy} where $TP$, $TN$, $FP$ and $FN$ are True/False Positives/Negatives.
    
    \begin{equation}\label{eq:eval-accuracy}
        Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
    \end{equation}
    
    Many complex networks are however quite sparse, such that simply predicting the absence of any links would give a fairly high accuracy. As an example, one of the datasets considered has $N=986$ entities with at most 24,929 links at a given time step, resulting in a sparsity of 97.4\%. To account for the class imbalance, a more appropriate measurement is AUC, known as the area under the ROC (Receiver Operating Characteristics) curve. The ROC curve is defined by plotting the true positive rate (TPR) against the false positive rate (FPR) at several classification thresholds, computed as shown in \Cref{eq:eval-tpr-fpr}. By lowering the classification threshold, more observations are classified as positive, thus increasing both the number of False Positives as well as True Positives.
    
    \begin{equation}\label{eq:eval-tpr-fpr}
        TPR = \frac{TP}{TP + FN} \qquad\qquad FPR = \frac{FP}{FP + TN}
    \end{equation}
    
    A sample ROC curve is depicted in \Cref{Method/AUC-ROC-Curve} as the solid blue line, with the area under the curve in light blue.
    The AUC metric indicates how well a model distinguishes between classes, where an AUC score of 0.5 corresponds to simply making random guesses, illustrated by the dashed line. As the model's AUC score increase, the better it is at predicting 0s as 0s and 1s as 1s, such that an AUC score of 1 corresponds to perfect class separation.
    
    \xfig{\ximg[width=0.7\textwidth]{Method/AUC-ROC-Curve}}

\section{Case-Control Approximate Likelihood}

    Due to the structure of the latent space model's likelihood function (eq.~\ref{eq:lsm-log-likelihood}), the computational complexity is $O(N^2)$, having to sum over $N(N-1)$ terms. This issue is studied in \cite{raftery2012fast} by \citeauthor{raftery2012fast}, where a case-control idea from epidemiology is used to estimate the full likelihood at a cost of only $O(N)$. This is achieved by exploiting the fact that large networks are usually sparse, having a small number of highly connected hub nodes while the majority of nodes have low degree.
    
    In epidemiology, case-control studies are used to compare a \emph{case} group having the outcome of interest to a \emph{control} group. The cases are often so rare that it is infeasible to draw a random sample with enough cases to draw conclusions.
    By considering the presence of links between entities as cases and absence of links as controls, determining which factors distinguish these two populations is similar to identifying the risk factors of disease in an epidemiological study.
    
    This analogy suggests an approximation to the log-likelihood function, which \citeauthor*{raftery2012fast} formalizes as
    \begin{equation}
    \label{eq:case-likelihood-approx}
        \ell \equiv \log P(Y|\eta)=\sum_{i=1}^n \ell_i
    \end{equation}
    where
    \begin{equation}
        \setlength{\jot}{0.5\baselineskip}
        \begin{split}
            \ell_i &\equiv \sum_{i\neq j}\{\sub(\eta)\sub(y)-\log(1+e^{\sub(\eta)})\} = \sub(\ell)(i,1) + \sub(\ell)(i,0)\\
            &= \sum_{i\neq j, \sub(Y)=1} \{\sub(\eta)-\log(1+e^{\sub(\eta)}) \} + \sum_{i\neq j, \sub(Y)=0} \{-\log(1+e^{\sub(\eta)}) \} \\
            % &= \sub(\ell)(i,1) + \sub(\ell)(i,0)
        \end{split}
    \end{equation}
    such that \Cref{eq:case-likelihood-approx} sums over the rows of the sociomatrix $Y$. The quantity $\sub(\ell)(i,0)$ can be viewed as a population total statistic, which is estimated by a random sample of the population:
    \begin{equation}
        \sub(\tilde{\ell})(i,0)=\frac{\sub(N)(i,0)}{\sub(n)(i,0)} \sum_{k=1}^{\sub(n)(i,0)} \{ -\log(1+e^{\sub(\eta)(ik)}) \}
    \end{equation}
    where $\sub(N)(i,0)$ is the total number of 0s in the $i$th row and $\sub(n)(i,0)$ is the number of samples selected from the $i$th row, summarizing the selected entries. Since $\sub(\tilde{\ell})(i,0)$ is based on a random sample from the 0s, $E[\sub(\tilde{\ell})(i,0)] = \sub(\ell)(i,0)$. Hence, the computation needed to estimate $\sub(\ell)(i,0)$ can be reduced significantly for large networks by choosing a relatively small $\sub(n)(i,0)$.
    
    One drawback, is the fact that this estimator does not consider the \emph{closeness} of nodes, while the latent space model assumes that nodes closer in latent space are more likely to form links than those far apart. As a result, the population of 0s is not homogeneous, where the nodes closest to node $i$ are more relevant when estimating its latent position. To remedy this, the shortest path length from node $i$ to node $j$ in the network is precomputed, $\sub(D)$, as a measure of closeness. Then, a stratified sampling approach is used, dividing the 0s into $M$ strata according to $\sub(D)$, leading to the decomposition of the contribution to the log-likelihood in \Cref{eq:case-stratified-likelihood}.
    \begin{equation}
    \label{eq:case-stratified-likelihood}
        \setlength{\jot}{0.5\baselineskip}
        \begin{split}
            \ell_i = & \sum_{j:\sub(Y)=1} \{\sub(\eta)-\log(1+e^{\sub(\eta)}) \} + \sum_{j:\sub(D)=2} \{-\log(1+e^{\sub(\eta)}) \} \\
                     & + \ldots + \sum_{j:\sub(D)=M} \{-\log(1+e^{\sub(\eta)}) \}
        \end{split}
    \end{equation}
    such that an unbiased estimator of $\ell_i$ based on stratified sampling is defined as
    \begin{equation}
    \label{eq:case-stratified-estimate}
        \hat{\ell}_i = \sum_{j:\sub(Y)=1} \{\sub(\eta)-\log(1+e^{\sub(\eta)}) \} + \sum_{h=2}^M \frac{\sub(N)(i,h)}{\sub(n)(i,h)} \sum_{j:\sub(D)=h} \{-\log(1+e^{\sub(\eta)}) \}
    \end{equation}
    where $\sub(N)(i,h)$ is the total number of nodes $j$ with $\sub(D)=h$ and $\sub(n)(i,h)$ is the number of selected samples in the $h$th stratum.
    
    The only remaining task is to determine $\sub(n)(i,h)$ for each value of $h$. 
    This is done by picking a global control-to-case rate $r$ and setting the total control size of each node $\sub(n)(i,h)=r\bar{d}\equiv n_0$, where $\bar{d}$ is the mean degree of the entire network. Having a fixed value of $\sub(n)(i,0)=\sum_{h=1}^M\sub(n)(i,h)$, the number of samples selected in each stratum $\sub(n)(i,h)$ is set to be proportional to the $h$th stratum's contribution to the change in log-likelihood when sampling latent positions.
    
    \citeauthor{raftery2012fast} does so by conducting a pilot MCMC run, where the change in log-likelihood is calculated at each iteration and the values of $\sub(n)(i,h)$ are updated accordingly. Hence, the stratified case-control log-likelihood is computed by summing over $O(n_0)$ terms, which does not grow with the network size $N$. Typically $n_0$ is small compared to $N$, thus resulting in a computational cost reduction from $O(N^2)$ to $O(N)$.
    
    % \begin{equation}\label{eq:case-likelihhod-change}
    %     \def\z{(z_i^{(t)})}
    %     \def\zs{(z_i^{(t)*})}
    %     \Delta\tilde{\ell}_i^{(t)}\equiv \tilde{\ell}_i\zs-\tilde{\ell}_i\z=\sub(\ell)(i,1)\zs-\sub(\ell)(i,1)\z + \sum_h\{\sub(\tilde{\ell})(i,h)\zs-\sub(\tilde{\ell})(i,h)\zs-\sub(\tilde{\ell})(i,h)\z\}
    % \end{equation}
    
\section{Model Optimization}

    The latent space positions and model parameters are estimated using maximum likelihood estimation. The inference for the latent space model introduced by \citeauthor*{hoff2002latent} and for the case-control approximate likelihood by \citeauthor*{raftery2012fast} is based on Markov chain Monte Carlo (MCMC) algorithms, while the dynamic model developed by \citeauthor{sarkar2005dynamic} uses KD-trees as an adaptation of multidimensional scaling. Here, we consider an alternative approach presented by \citeauthor{jacobsen2018a} for static latent space models, adapting the method to account for the temporal aspect. The main advantage is that this approach facilitates the use of GPU computability, allowing for analysis of much larger networks.

    \subsection{Gradient Descent}
    
        The open-source \href{https://ml-cheatsheet.readthedocs.io/en/latest/index.html}{Machine Learning Cheatsheet} website describes gradient descent very succinctly as follows:
        \begin{displayquote}\itshape
            Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient. 
        \end{displayquote}
        
        To understand how this is achieved, \citeauthor{nielsen2018neural} introduces the concept using an example cost function in \cite{nielsen2018neural}.
        Consider a function $C(v)$ of many variables $v = v_1, v_2, \dots$ shaping a multi-dimensional landscape. For a function of just two variables, the landscape can be illustrated in a three-dimensional plot as shown in \Cref{Method/Gradient-Descent}. 
        \xfig{\ximg{Method/Gradient-Descent}}
        Starting at an arbitrary point on the graph, the goal is to find the values for $v_1$ and $v_2$ where $C$ achieves its global minimum. One way of doing so, is computing the derivatives and find for which pair of $(v_1,v_2)$ values $C$ is an extremum, analyzing the given minima and maxima for a global minimum. This approach does however quickly become computationally infeasible as the number of variables increases. 
        
        While the derivatives cannot be used directly to show where the minimum is, they can tell us exactly how the graph is shaped at that specific point. 
        Knowing this, the optimizer can move a small amount $\Delta v_1$ in the $v_1$ direction, and a small amount $\Delta v_2$ in the $v_2$ direction, for which \Cref{eq:mopt-DeltaC} defines the change in $C$.
        \begin{equation}\label{eq:mopt-DeltaC}
            \Delta C \approx \frac{\partial C}{\partial v_1}\Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2
        \end{equation}
        
        The only task is then to find a way of choosing $\Delta v\equiv (\Delta v_1, \Delta v_2)^T$ making $\Delta C$ negative, as we aim to minimize $C$. Consider the gradient of $C$ to be the vector of partial derivatives, defined in the following equation.
        \begin{equation}\label{eq:mopt-gradient}
            \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}\right)^T
        \end{equation}
        
        Now, $\Delta C$ can be rewritten as
        \begin{equation}\label{eq:mopt-DeltaC-gradient}
            \Delta C \approx \Delta v \cdot \nabla C
        \end{equation}
        which shows how to choose $\Delta v$ with respect to making $\Delta C$ negative. By introducing a small positive parameter $\eta$, known as the step size or learning rate, $\Delta v$ is selected as follows
        \begin{equation}
            \Delta v = -\eta \nabla C
        \end{equation}
        such that 
        \begin{equation}
            \Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta \Vert\nabla C\Vert^2
        \end{equation}
        
        This guarantees that $\Delta C \leq 0$ as $\Vert\nabla C\Vert^2 \geq 0$. Hence, $C$ will always decrease, taking steps toward its minimum. 
        
        \subsubsection{Learning Rate}
        
            The learning rate $\eta$ controls how big steps the optimizer takes down the slope, where a too large step may result in moving past the minimum. On the contrary, a too small step size makes $\Delta C$ so tiny that the optimization algorithm barely makes progress. There are several techniques to facilitate the convergence of optimizers, including separate learning rates for different parameters and adaptive learning rates.
            
            Using an adaptive learning rate can be highly beneficial, starting with a large step size to quickly approach a solution, and then slowly decay to fine tune the parameters. Other uses involve schedules, adjusting $\eta$ when reaching a specific evaluation score, or when the optimizer has reached a plateau.
        
        \subsubsection{Optimizers}
        
            Note that the landscape defined by the cost function $C$ is usually a lot more rigid, having several minima, maxima, saddle points and plateaus. Hence, finding the global minimum is still a non-trivial task, where several adaptations of the gradient descent have been developed with various extensions to overcome vanishing gradients and local minima issues. 
            \citeauthor{ruder2017overview} provides an overview of gradient descent optimization algorithms in \cite{ruder2017overview}.
            
            In particular, Adam (Adaptive Moment Estimation) is a state-of-the-art optimizer, including several key additions. As the name suggests, the algorithm computes adaptive learning rates for each parameter. Furthermore, the method stores an exponentially decaying average of past squared gradients, as well as a decaying average of past gradients, similar to momentum.
            
            Momentum is an analogy from physics, where a ball going downhill accelerates and rolls past flat minima until it is stopped by friction. By using the same concept in gradient descent, the optimizer is less prone to end up in local minima, while the global minimum is presumably too steep for the ball to escape from.

        \subsubsection{Cost Functions}
        
            There are several popular cost functions (also referred to as objective functions or loss functions), and which function to use often depends on the problem the optimizer is attempting to solve. 
            In terms of a linear regression problem, the goal involve finding a linear mapping between input and output, such that the cost (or loss) should be minimum for the line of best fit. Hence, an appropriate cost function penalizes the difference between the target output $y$ and the predicted output $\hat{y}$ given by the line. 
            There are several ways of penalizing this difference, and some of the most common measures are \emph{Mean Aboslute Error (MAE)} and \emph{Mean Squared Error (MSE)}. The latter is shown in \Cref{eq:mopt-mseloss} as an example.
            \begin{equation}\label{eq:mopt-mseloss}
                MSE = \frac{1}{N}\sum_{i=1}^N(y_i-\hat{y}_i)^2
            \end{equation}
            
            For classification tasks the essence is similar, however the cost function is quite dissimilar. The biggest difference is that the predicted output $\hat{y}$ is not a quantity, but a probability. Knowing that a probability ranges from zero to one, we can use the fact that the logarithm becomes increasingly negative as $\hat{y}\rightarrow0$. Then, by taking the negative of the logarithm, we have a function that increases as $\hat{y}$ becomes less certain, depicted in \Cref{Method/Negative-Log-Probability}.
            \xfig{\ximg[width=0.7\textwidth]{Method/Negative-Log-Probability}}
            This is the foundation of the most common cost function for classification problems, which is referred to as \emph{Binary Cross-Entropy (BCE)} for the two-class scenario, shown in \Cref{eq:mopt-bceloss}.
            \begin{equation}\label{eq:mopt-bceloss}
                BCE = -\frac{1}{N}\sum_{i=1}^N y_i\log(\hat{y}_i)+(1-y_i)\log(1-\hat{y}_i)
            \end{equation}
            The first term $y_i\log(\hat{y}_i)$ penalizes a low probability $\hat{y}_i$ when the target $y_i$ is 1, while the second term penalizes a high probability when the target is 0. As a result, the cost function facilitates class separation when minimized by an optimizer.
        
        \subsubsection{Stopping Criteria}
        
            The last topic worthy of mentioning is stopping criteria, deciding when to stop the optimization procedure. This is most often used to prevent overfitting, as training 
    
\section{PyTorch Implementation}

    \subsection{Pairwise Distance}
    
    \subsection{Latent Space Positions}
    
        One of the most time-consuming tasks is the computation of the latent space positions, particularly in terms of the autoregressive process where the latent positions at time $T$ depend on all $T-1$ previous latent spaces.
        There are two key challenges to keep in mind, namely keeping the PyTorch computation graph intact as well as keeping the optimization procedure well-behaved. 
        %This entails that each step of the optimizer contributes to a decrease in the overall loss, and that the gradients will eventually converge to zero.
        
        Starting with a subset of nodes (also called a $batch$) at a particular time $t$, the predictions $\sub(p)\text{ for } i,j \in batch$ are computed, assigning high and low probabilities depending on the bias $\beta$, the latent positions $Z$ and other model parameters. These predictions are compared to the true results, indicating which nodes are actually linked, resulting in a $loss$ which is inversely proportional to the correctness of the model. Every single step, from the batch is drawn to the loss is computed, is recorded in PyTorch's computation graph, telling exactly which operations were performed to get from batch to result. 
        
        Now, to ensure better predictions at the next iteration, PyTorch propagates backward through the recorded graph, computing the gradients with respect to the latent positions and the other parameters. Doing so, the optimizer knows which parameters to increase and decrease, including which direction to move the latent positions.
        The caveat is, that if the chain of operations is broken, the optimizer will have no way of computing the gradients with respect to the adjustable parameters. 
        
        \subsubsection{Storing Intermediate Positions}
            
            The most obvious solution is to simply recompute everything for each batch and time step, although this seems quite redundant considering the autoregressive model has to compute the latent positions for all time steps $0\leq t<T$ before computing the positions at time $T$. Hence, it is tempting to store the computed latent space positions as we iterate through the time steps sequentially. Doing so speeds up the computation significantly, however the parameters are no longer optimized properly. By utilizing these intermediary latent positions, the resulting positions are no longer directly related to the optimizable initial positions, not using the full computation graph.
        
        \subsubsection{Pre-computing Latent Spaces}
            
            An alternative is to compute all latent positions for all time steps beforehand, and simply pass the correct latent space representation to the appropriate step of the optimization procedure, thus keeping the computation graph intact. This is possible by specifying that the computation graph should be retained between backward passes. However, the computation graph is still quite large for higher-order autoregressive models, and more importantly, doing so leads to serious behavioral issues in the optimization procedure.
            
        \subsubsection{Optimization Procedure}
        
            The model optimization procedure relies on one main assumption, namely that the parameters eventually converge to some value. regardless of being a global or local minima.
            
            
            
        \subsubsection{Block Optimization}
        
    \subsection{Estimating Innovation}
    
        In time series analysis, the \emph{innovation} is the difference between the observed value of a variable at time $t$ and the optimal forecast of that value based on information available prior to time $t$
            
    \subsection{Operation Profiling}
    
        All operations are timed seven runs with 1.000-100.000 loops each depending on the run-time of the operation, showing timings as \verb|mean ± std|.
    
        \subsubsection{Indexing}
        
        \begin{verbatim}
%timeit E[:,idx]
%timeit E.index_select(1,LongTensor(idx))
%timeit FloatTensor(1,len(idx),2)
        \end{verbatim}
        \begin{verbatim}
642 µs ± 5.82 µs
72.5 µs ± 987 ns
2.3 µs ± 19.4 ns
        \end{verbatim}
        
        
        \begin{verbatim}
%timeit E[:,idx].normal_() * E_std
%timeit E.index_select(1,LongTensor(idx)).normal_() * E_std
%timeit FloatTensor(1,len(idx),2).normal_() * E_std
%timeit torch.normal(FloatTensor(1,len(idx),2).fill_(0), \
                        E_std.expand((1,len(idx),2)))
mean = FloatTensor(1,len(idx),2).fill_(0)
std = E_std.expand((1,len(idx),2))
%timeit torch.normal(mean, std)
        \end{verbatim}
        \begin{verbatim}
746 µs ± 11.8 µs
163 µs ± 2.47 µs
108 µs ± 1.39 µs
85.6 µs ± 1.64 µs
27.7 µs ± 490 ns
        \end{verbatim}
        
    
Som jeg forstår på den kode jeg så i forbindelse med vores Skype møde sampler du fra  (prior) fordelingen støjen, i.e.


$e_t\sim N(0,diag(\sigma^2_e))$

og lærer hvad $\sigma^2_e$ er som del af din inferens.


Dette svarer til i modellen

$p(A,e|\theta,z)= p(A|\theta,z,e)p(e|\sigma^2_e) $
hvor A er dit netværk, theta dine AR parametre,z de latente positioner og e støj innovationerne, at du forsøger at marginalisere e ved at approksimerer denne marginalisering med et enkelt sample fra $p(e|\sigma^2_e)$ , i.e.


$p(A|\theta,z)=\int p(A|\theta,z,e)p(e|\sigma^2_e)  de \approx p(A|\theta,z,e_s)$

hvor $e_s\sim p(e|\sigma^2_e)$. 



Dette er som jeg ser det helt OK, men en anden mulighed var at forsøge at inferere også E, i.e. minimere:

$-log(p(A,e|\theta,z))=-log( p(A|\theta,z,e)p(e|\sigma^2_e) )=-log( p(A|\theta,z,e))-log(p(e|\sigma^2_e) )$

Hvor det første led er krydsentropien og det andet led den associerede omkostning ved at skulle tilføje støj. Ved ikke om dette potentielt kan være en fordel i forhold til inferensen. Du kunne overveje at undersøge begge muligheder.



Når du prædiktere skal du stadig sample e som du hidtil har gjort fra $p(e|\sigma^2_e)$.