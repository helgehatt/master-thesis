\chapter{Discussion}\label{ch:Discussion}

\section{Hyperparameters}

The model converges much faster training on smaller batches at the time. Doing so, the optimizer tunes a limited number of parameters at each iteration, instead of making changes to all parameters, possibly making several mistakes.

The learning rate has a huge impact on the time it takes the model to converge, benefiting from an adaptive learning rate schedule to quickly get close to the convergence limit before reducing the step size.


Accumulating gradient