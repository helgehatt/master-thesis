\chapter{Discussion}\label{ch:Discussion}

This chapter discusses the results of \Cref{ch:Results}, as well as main observations on how the different variations of models affect the final scores and learned latent representations. The chapter is concluded with a discussion of several possible extensions, and how to proceed from here.

\section{Summary}

    The results of the previous chapter are summarized, starting with the synthetic data. 

    \subsection{Synthetic Data}
    
        The synthetic datasets show how the latent models are capable of learning the underlying network structure, including the benefits of advanced dynamic models. Knowing that the diffusion added in the synthetic datasets follow a Gaussian distribution, the LL diffusion penalty corresponding to a Laplace distribution is not considered.
        
        \subsubsection{Static Data}
        
            The static data represent a non-temporal network, where the naive approach of simply using the same latent positions for all time steps excels. The dynamic models are trained using no diffusion penalty, similar to the static latent space model. Considering that the dataset includes no diffusion, penalizing diffusion immensely would likely produce better results, however this is done to emphasize the difference between the static and dynamic models.
            
            The true underlying network representation achieves an AUC score of 91\%, with the static latent space model at 90\%, followed by the dynamic models at 87-88\%. Now why is the true representation unable to achieve a score of 100\%? This is because the true positions are only used to derive a set of link probabilities, from which the links in the static dataset are drawn. This is why it is necessary to compare the trained models with the true model, representing the highest possible score a model can achieve.
        
        \subsubsection{Dynamic Data}
        
            Considering that the underlying true network simulates a diffusion process, the dataset is first analyzed in terms of the diffusion model, using different diffusion penalties. Looking at the convergence animations on \cpageref{Results/Dynamic/Train/DiffusionModel-AR(1)-Undirected-NoLoss}, a few observations are made. Firstly, using no diffusion penalty results in overfitting, where there is a large deviation in score from training data to test data. Secondly, the Gaussian cost function penalizes diffusion so much that the underlying movement is not learned. As a result, the Gaussian penalty without a variance parameter provides the best fit, achieving a balance between too much and too little diffusion.
            
            The true representation achieves an AUC score of 92\%, followed by the diffusion model at 90\%. The autoregressive models are quite capable of capturing the dynamicity of the data, however with a slightly more complicated pattern, resulting in scores at 87-89\%. The static latent space model struggles with the temporal change, such that the learned latent space is a mixture of the true initial and final positions, with a score of 84\%. 
        
        \subsubsection{Periodic Data}
        
            The periodic dataset is the one that truly separates the autoregressive models, having three sets of positions rotating in a periodic fashion. Similar to the dynamic data, the convergence of the AR(3) model is depicted on \cpageref{Results/Periodic/Train/AutoregressiveModel-AR(3)-Undirected-NoLoss}. Recall that autoregressive models are optimized using the warm start principle (cf. \cref{sec:pyimpl-positions}), which is quite apparent in the animations. The Gaussian penalty produces accurate results, both with and without a variance parameter. Given an insignificant superiority of 0.1\%, the penalty with the variance parameter is chosen for comparison.
            
            Looking at the ROC curves in \Cref{Results/Periodic/AUC-Undirected-GL}, it really shows that the DFM and LSM are in fact AR(1) models with and without diffusion, respectively. The true representation obtains a score of 90\%, followed by the AR(3) model at 89\%, the AR(2) model at 80\% and finally the AR(1) models at 68-69\%. The AR(2) model achieves a much higher score than the AR(1) models, being able to model two sets of positions, opposed to only one.
        
    \subsection{Synthetic AR(2) Data}
    
        Several experiments have been conducted with synthetic AR(2) datasets, as the second order autoregressive process in theory is capable of modeling velocity, acceleration and curvature. This included datasets with clusters forming new clusters, and even with a cluster moving across a grid of nodes. 
        Velocity is modeled as constant change in latent positions between time steps, using $\bmphi=[2,-1]$ (for all $k$ dimensions). Doing so adds the difference $\Z_t - \Z_{t-1}$ to the most recent set of observations $\Z_t$, corresponding to the autoregressive process shown in \Cref{eq:data-velocity}. The diffusion $\bmepsilon$ is omitted for simplicity.
        \begin{equation}\label{eq:data-velocity}
            \Z_t = 2 \Z_{t-1} - \Z_{t-2} = \Z_{t-1} + (\Z_{t-1} - \Z_{t-2})
        \end{equation}
        The distance between the two sets of initial latent positions then define the magnitude of the change, or simply put, the speed. With minor adjustments to the AR coefficients, the process can model acceleration or curvature instead. However, the difficult part is to learn the exact combination of coefficients and initial positions producing the correct movement. Generating small synthetic networks comprised of only 30 nodes, all attempts have resulted in either a static fit or a dynamic fit to the training data. While this may be due to lack of structure related to the network size, the result poses another issue. There are several combinations of AR coefficients producing an accurate fit to the training data, without generalizing to the test data. 
        This raises a question of whether or not modeling velocity, acceleration and curvature is feasible in practice, and a more thorough analysis must be carried out.

    \subsection{EU Email Data}
    
        Not knowing the underlying network structure, it is difficult to distinguish between a good and bad latent representation. Fortunately, the ROC curves are quite transparent, showing the AUC score of each individual model. Comparing undirected and directed models using Gaussian penalty, the undirected models produce slightly better results for $k=2$, while the directed AR models improve a lot (5\% increase) by adding another dimension. Using ten-dimensional latent positions, both the undirected and directed models perform equally well, with scores as high as 97-98\%, indicating that the assumption of reciprocity holds. The significant performance increase from two to three dimensions gives reason to believe that the directed models are capable of reaching their full potential at a lower $k$ than the undirected models. This could be further analyzed.
        
        Inspecting the results using LL and GNV penalties, the scores for K(2) suggests that the directed GL models are not fully converged, despite several reruns. Besides the K(2) results, the performance for K(3) and K(10) are quite similar, with a slight edge to GL in terms of K(3) and LL in terms of K(10). However, the differences are not large enough to draw a conclusion on which penalty is superior. A more surprising result, is the fact that the static latent space model performs as well as, or even slightly better than the dynamic models. This is in spite of the distinct periodic pattern related to weekdays and weekends, for which the AR(7) model captures no additional information. 
        
        It is important to note that there is a huge difference between predicting the number of links present at each time step, and predicting exactly which of the thousand actors are linked at which time. The first problem would readily be solved by the AR(7) model, while the static LSM is apparently just as suited to solve the second problem. This raises a question of whether or not the autoregressive processes are simply modeling a static snapshot, disregarding rotation, reflection and scaling.
        
    \subsection{UC Messaging Data}
    
        Remove inactive nodes
    
\section{Model Variations}

    \subsection{Static vs Dynamic}

    \subsection{Undirected vs Directed}
    
        Undirected models have a very distinct latent space representation, with observations scattered around a dense core. This gives reason to believe that the network's actors are ranked according to their activity, with the most active actors centered at the origin. 
        Directed models show similar behavior, a
    
    \subsection{\texorpdfstring{$k$}{k}-Dimensional}
    
    \subsection{Autoregressive Order}
    
    \subsection{Diffusion Penalty}
    
        From the synthetic data, it was observed that using no diffusion penalty resulted in overfitting, and worse performance in general, regardless of whether the underlying data included little or a lot of diffusion. Furthermore, when the temporal changes consisted solely of diffusion, the Gaussian penalty limited the model convergence. However, with a small amount of diffusion, 
    
\section{Extensions}
    
    \subsection{Hyperparameter Tuning}
    
        Optimizing hyperparameters has not been a primary concern, 
    
    \subsection{Parallelization}
    
        Parallelizing the optimization procedure was one of the initial project goals, having access to the university's GPU cluster.
        However, the current bottleneck is not GPU utilization, but the iterative computation of the autoregressive latent positions. The LSM and DFM are in fact already capable of running on networks with more than 10,000 nodes, converging faster than AR models on networks with only 1,000 nodes.

    \subsection{Direct AR Computation}
    
        Direct AR computation refers to the LSM and DFM models' ability to compute the latent positions at time $t$ directly from the initial positions. Doing so, the latent positions at $t=500$ are computed just as quickly as the positions at $t=5$. This is not the case for the AR models, where calculating the positions at $t=500$ requires the computation of all 499 previous sets of positions, thus scaling linearly with $t$. This is a critical limitation of the AR models, which must be solved before analyzing networks over an increased period of time.
    
    \subsection{GPU Validation}
    
        Performing model validation during training is essential to control the warm start optimization for autoregressive models. With a direct approach to AR computation, warm starting would not necessary, since the time between parameter updates would no longer scale with the time step $t$. As a result, intermediate model validation could be avoided altogether, however, another solution is to use the \href{https://github.com/h2oai/h2o4gpu}{H2O4GPU} library, being a GPU implementation of the currently used scikit-learn API. GPU-based AUC scoring has been requested, and will likely be implemented in the near future.

    \subsection{Next Steps}
    
        % Math Overflow temporal network
        % Aggregate to monthly data
        
    \subsection{Procrustean Transformation}
    
        % Invariant under rotation, translation, reflection and scaling

% \section{Model Convergence}

% One of the most challenging tasks is getting the models to converge properly, which entails a high convergence limit, stable progress and as quickly as possible.

% Modeling each subsequent time step as its own set of positions versus relating each subsequent time step directly to the initial positions.

% Local minima, especially in terms of the autoregressive model, where small modifications to the AR coefficients have a huge impact.


% Training on the first few time steps before training on the rest.


% Tried accumulating gradients, however the issue was with the computation graph

% One of the biggest issues, including the initial positions in the computation graph

% \section{Parallelization}

%     A natural extension is to parallelize the GPU implementation.
    
%     %ensemble

% \section{Hyperparameters}

% The model converges much faster training on smaller batches at the time. Doing so, the optimizer tunes a limited number of parameters at each iteration, instead of making changes to all parameters, possibly making several mistakes.

% The learning rate has a huge impact on the time it takes the model to converge, benefiting from an adaptive learning rate schedule to quickly get close to the convergence limit before reducing the step size.



% Training on the last half of the training data yields much faster convergence
