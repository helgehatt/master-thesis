

\section{Synthetic Data}

The synthetic data is comprised of 30 nodes over 100 time steps, and is split into a training set of 80 time steps and a test set with the remaining 20. Knowing that the datasets are undirected and two-dimensional, directed models and models of higher dimensions are not considered. A trained model of correct type and order is expected to be able to accurately capture the underlying true model.

With a limited number of entities in the synthetic datasets, each batch include all nodes, where the loss of a batch is computed 80 times an epoch, once for each time step in the training data. Validation occurs every ten epochs, computing the AUC scores for each individual time step as well as average training and test AUCs.
The optimization procedure uses an early stopping criteria, where the evaluation metric is the equally weighted sum of average training and test AUCs. Termination occurs when the metric increased by less than 0.01 over three validation steps (30 epochs). In case of the autoregressive models, the training does not stop, but continues to the next optimization block as described in \Cref{sec:pyimpl-positions} on warm starting.

% The models are trained for a minimum of 200 epochs, using an early stopping criteria to decide when the training has converged. The requirement is an increase in test AUC of 0.001 over 30 epochs, terminating the optimization procedure if this criteria is not met. 

% When the training stops for the first time, the number of time steps to include in a single step of the optimizer increases from 1 to 10, continuing training with accumulated gradients to further improve performance. Since the change in diffusion rate and AR coefficients has an impact on all time steps, optimizing for a single time step can result in a negative impact overall. Hence, accumulating the gradients makes the training more stable, however it also makes the optimizer more prone to end in local minima, and including more than 10 time steps has not proved to benefit performance.

\subsection{Static Data}

    \Cref{Results/Static/Train/StaticModel-AR(1)-Undirected-NoLoss} depicts the convergence of the static latent space model, showing the average loss per batch (upper left), the average training and test AUC scores (lower left) as well as the model's current AUC scores for each individual time step (right). The convergence is shown for selected models, especially to provide insight into how the different types of models converge, and how the convergence is affected by for instance the use of different diffusion penalties.

    \xfig[H]{\xanim{Results/Static/Train/StaticModel-AR(1)-Undirected-NoLoss}[LSM | U | K(2) | NL]}
    
    The next page includes several figures and animations visualizing the true network and the latent representations generated by the different models. The goal of the models is to mimic the true network as well as possible, giving rise to accurate test predictions. The models produce link probabilities, and not actual binary predictions, such that the networks are depicted with edges of various grey tones, where a darker edge represents a higher link probability. This applies to the true network as well, illustrating the underlying link probabilities from which the links are drawn. The nodes are labeled with numbers, allowing a direct comparison of closeness between true positions and latent space positions.
    
    The synthetic data is concluded with a comparison of test ROC curves, and the corresponding AUC scores in the legend, for each individual model for each dataset. The static latent space model is expected to perform slightly better than the other models on the static data, as the dynamic models are likely to provide an overcomplicated fit. However, the dynamic and periodic data require the additional complexity, where the static model is expected to perform significantly worse.
    
    \xfig[H]{\xdouble
        {\ximg{Results/Static/Proba/StaticModel-AR(1)-Undirected-True}[True]}
        {\ximg{Results/Static/Proba/StaticModel-AR(1)-Undirected-NoLoss}[LSM | U | K(2) | NL]}
    }\xfig[H]{\xdouble
        {\xanim{Results/Static/Proba/DiffusionModel-AR(1)-Undirected-NoLoss}[DFM | U | K(2) | NL]}
        {\xanim{Results/Static/Proba/AutoregressiveModel-AR(1)-Undirected-NoLoss}[AR(1) | U | K(2) | NL]}
    }\xfig[H]{\xdouble
        {\xanim{Results/Static/Proba/AutoregressiveModel-AR(2)-Undirected-NoLoss}[AR(2) | U | K(2) | NL]}
        {\xanim{Results/Static/Proba/AutoregressiveModel-AR(3)-Undirected-NoLoss}[AR(3) | U | K(2) | NL]}
    }


\subsection{Dynamic Data}
    
    \xfig[H]{\xdouble
        {\xanim{Results/Dynamic/Train/DiffusionModel-AR(1)-Undirected-NoLoss}[DFM | U | K(2) | NL]}
        {\xanim{Results/Dynamic/Space/DiffusionModel-AR(1)-Undirected-NoLoss}[DFM | U | K(2) | NL]}
    }\xfig[H]{\xdouble
        {\xanim{Results/Dynamic/Train/DiffusionModel-AR(1)-Undirected-GaussianLoss}[DFM | U | K(2) | GL]}
        {\xanim{Results/Dynamic/Space/DiffusionModel-AR(1)-Undirected-GaussianLoss}[DFM | U | K(2) | GL]}
    }\xfig[H]{\xdouble
        {\xanim{Results/Dynamic/Train/DiffusionModel-AR(1)-Undirected-GaussianLossNoVar}[DFM | U | K(2) | GNV]}
        {\xanim{Results/Dynamic/Space/DiffusionModel-AR(1)-Undirected-GaussianLossNoVar}[DFM | U | K(2) | GNV]}
    }\xfig[H]{\xdouble
        {\xanim{Results/Dynamic/Proba/DiffusionModel-AR(1)-Undirected-True}[True]}
        {\ximg {Results/Dynamic/Proba/StaticModel-AR(1)-Undirected-NoLoss}[LSM | U | K(2) | NL]}
    }\xfig[H]{\xdouble
        {\xanim{Results/Dynamic/Proba/DiffusionModel-AR(1)-Undirected-GaussianLossNoVar}[DFM | U | K(2) | GNV]}
        {\xanim|6|{Results/Dynamic/Proba/AutoregressiveModel-AR(1)-Undirected-GaussianLossNoVar}[AR(1) | U | K(2) | GNV]}
    }\xfig[H]{\xdouble
        {\xanim|6|{Results/Dynamic/Proba/AutoregressiveModel-AR(2)-Undirected-GaussianLossNoVar}[AR(2) | U | K(2) | GNV]}
        {\xanim|6|{Results/Dynamic/Proba/AutoregressiveModel-AR(3)-Undirected-GaussianLossNoVar}[AR(3) | U | K(2) | GNV]}
    }

\subsection{Periodic Data}
    
    \xfig[H]{\xdouble
        {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(3)-Undirected-NoLoss}[AR(3) | U | K(2) | NL]}
        {\xanim|3|{Results/Periodic/Space/AutoregressiveModel-AR(3)-Undirected-NoLoss}[AR(3) | U | K(2) | NL]}
    }\xfig[H]{\xdouble
        {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(3)-Undirected-GaussianLoss}[AR(3) | U | K(2) | GL]}
        {\xanim|3|{Results/Periodic/Space/AutoregressiveModel-AR(3)-Undirected-GaussianLoss}[AR(3) | U | K(2) | GL]}
    }\xfig[H]{\xdouble
        {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(3)-Undirected-GaussianLossNoVar}[AR(3) | U | K(2) | GNV]}
        {\xanim|3|{Results/Periodic/Space/AutoregressiveModel-AR(3)-Undirected-GaussianLossNoVar}[AR(3) | U | K(2) | GNV]}
    }\xfig[H]{\xdouble
        {\xanim|6|{Results/Periodic/Proba/AutoregressiveModel-AR(3)-Undirected-True}[True]}
        {\ximg{Results/Periodic/Proba/StaticModel-AR(1)-Undirected-NoLoss}[LSM | U | K(2) | NL]}
    }\xfig[H]{\xdouble
        {\xanim|6|{Results/Periodic/Proba/DiffusionModel-AR(1)-Undirected-GaussianLoss}[DFM | U | K(2) | GL]}
        {\xanim|6|{Results/Periodic/Proba/AutoregressiveModel-AR(1)-Undirected-GaussianLoss}[AR(1) | U | K(2) | GL]}
    }\xfig[H]{\xdouble
        {\xanim|6|{Results/Periodic/Proba/AutoregressiveModel-AR(2)-Undirected-GaussianLoss}[AR(2) | U | K(2) | GL]}
        {\xanim|3|{Results/Periodic/Proba/AutoregressiveModel-AR(3)-Undirected-GaussianLoss}[AR(3) | U | K(2) | GL]}
    }

    % \begin{figure}[!htb]
    %     \floatname{animation}{A}
    %     \xdicol{
    %         \xtricol
    %             {\xtrirow
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(1)-Undirected-NoLoss}[AR(1) | NL]}
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(2)-Undirected-NoLoss}[AR(2) | NL]}
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(3)-Undirected-NoLoss}[AR(3) | NL]}
    %             }{\xtrirow
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(1)-Undirected-GaussianLoss}[AR(1) | GL]}
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(2)-Undirected-GaussianLoss}[AR(2) | GL]}
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(3)-Undirected-GaussianLoss}[AR(3) | GL]}
    %             }{\xtrirow
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(1)-Undirected-GaussianLossNoVar}[AR(1) | GNV]}
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(2)-Undirected-GaussianLossNoVar}[AR(2) | GNV]}
    %                 {\xanim|6|{Results/Periodic/Space/AutoregressiveModel-AR(3)-Undirected-GaussianLossNoVar}[AR(3) | GNV]}
    %             }
    %         }{\xdicol
    %             {\xtrirow
    %                 {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(1)-Undirected-GaussianLoss}[AR(1) | GL]}
    %                 {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(2)-Undirected-GaussianLoss}[AR(2) | GL]}
    %                 {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(3)-Undirected-GaussianLoss}[AR(3) | GL]}
    %             }{\xtrirow
    %                 {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(1)-Undirected-GaussianLossNoVar}[AR(1) | GNV]}
    %                 {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(2)-Undirected-GaussianLossNoVar}[AR(2) | GNV]}
    %                 {\xanim{Results/Periodic/Train/AutoregressiveModel-AR(3)-Undirected-GaussianLossNoVar}[AR(3) | GNV]}
    %             }
    %         }
    % \end{figure}

\subsection{Comparison}

    \xfig[H]{\ximg[width=0.55\textwidth]{Results/Static/AUC-Undirected-NL}[Static Data | Test | U | K(2) | NL]}
    \xfig[H]{\ximg[width=0.55\textwidth]{Results/Dynamic/AUC-Undirected-GNV}[Dynamic Data | Test | U | K(2) | GNV]}
    \xfig[H]{\ximg[width=0.55\textwidth]{Results/Periodic/AUC-Undirected-GL}[Periodic Data | Test | U | K(2) | GL]}
