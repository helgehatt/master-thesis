\section{PyTorch Implementation}

The models and the MLE framework are implemented in PyTorch, an open-source deep learning platform aiming to provide a seamless path from research prototyping to production deployment.

\subsection{Pairwise Distance}

    Calculating link probabilities for a subset of nodes requires the computation of the pairwise distance matrix $D$, where $\sub(d)^2=\Vert z_i-z_j\Vert^2$ is the squared euclidean distance between nodes $i$ and $j$. The squared euclidean distance differs from euclidean distance by omitting the square root for faster clustering. The distance matrix is symmetric for an undirected latent space, while a directed latent space has separate representations for incoming and outgoing edges. In the latter case, $D$ is calculated from the latent space positions $Z_{in}$ and $Z_{out}$ such that $\sub(d)\neq\sub(d)(ji)$.
    
    Considering two arbitrary matrices $A$ and $B$ of size $N\times k$, the direct approach to computing the distance matrix is by adding extra dimensions such that the size of $A$ is $N\times 1\times k$ and $B$ is $1\times N\times k$. The squared differences between all combinations of rows and columns are then computed simultaneously as $(A-B)^2$, resulting in a $N\times N\times k$ matrix. The final step is to sum over the last dimension, giving rise to the $N\times N$ distance matrix. This approach is efficient for very small matrices, however scales poorly both in terms of memory consumption and computational complexity.
    
    A less memory intensive approach is to calculate the squared norm of the difference using quadratic expansion, shown in \Cref{eq:pyimpl-dist-quadratic}.
    \begin{equation}\label{eq:pyimpl-dist-quadratic}
        \sub(d)^2=\Vert z_i-z_j\Vert^2\equiv\Vert z_i\Vert^2 + \Vert z_j\Vert^2 - 2(z_i\cdot z_j)
    \end{equation}
    The squared norm of a matrix $A$ is computed by squaring all $N\times k$ entries and summing over the last dimension, resulting in a vector of size $N$. Viewing $\Vert A\Vert^2$ as a column vector and $\Vert B\Vert^2$ as a row vector, the full $N\times N$ distance matrix is computed as $\Vert A\Vert^2 + \Vert B\Vert^2 - 2AB^T$. Doing so, the memory consumption is $O(N^2)$ for the final distance matrix (assuming $k\ll N$), down from $O(N^2k)$ for the intermediary results of the direct approach.
    
    To illustrate the difference in computational complexity between the two methods, \Cref{Method/Pairwise-Distance-N} shows the time it takes to compute the calculations (in Âµs) as a function of $N$ and as a function of $k$ in \Cref{Method/Pairwise-Distance-k}. The timings are the mean of 7 runs with 100-1000 loops each, carried out on an RTX-2080 GPU. Both the direct approach and the approach utilizing quadratic expansion scale exponentially with $N$, however with a much lower rate in terms of the second. Additionally, the first method scales linearly with $k$, while the second remains constant. Hence, empirical tests show that using quadratic expansion is a much more efficient way of computing the pairwise distances as the number of nodes in a batch and the dimensionality of the latent space increases.
    \xfig{\xdirow
        {\ximg{Method/Pairwise-Distance-N}}
        {\ximg{Method/Pairwise-Distance-k}}
    }
    
\subsection{Latent Space Positions}

    One of the most time-consuming tasks is the computation of the latent space positions, particularly in terms of the autoregressive process where the latent positions at time $t$ depend on all $t-1$ previous latent spaces.
    There are two key challenges to keep in mind, namely keeping the PyTorch computation graph intact as well as keeping the optimization procedure well-behaved. 
    %This entails that each step of the optimizer contributes to a decrease in the overall loss, and that the gradients will eventually converge to zero.
    
    Starting with a subset of nodes ($batch$) at a particular time $t$, the predictions $\sub(p)\text{ for } i,j \in batch$ are computed, assigning high and low probabilities depending on the bias $\beta$, the latent positions $Z$ and other model parameters. These predictions are compared to the true results, indicating which nodes are actually linked, resulting in a $loss$ which is inversely proportional to the correctness of the model. Every single step, from the batch is drawn to the loss is computed, is recorded in PyTorch's computation graph, telling exactly which operations were performed to get from batch to result. 
    
    Now, to ensure better predictions at the next iteration, PyTorch propagates backward through the recorded graph, computing the gradients with respect to the latent positions and the other parameters. Doing so, the optimizer knows which parameters to increase and decrease, including which direction to move the latent positions.
    The caveat is, that if the chain of operations is broken, the optimizer will have no way of computing the gradients with respect to the adjustable parameters. 
    
    \subsubsection{Storing Intermediate Positions}
        
        The most obvious solution is to simply recompute everything for each batch and for each time step. Doing so is however quite redundant considering the autoregressive model has to compute the latent positions for all time steps $0\leq t<T$ before computing the positions at time $T$. Hence, it is tempting to store the computed latent space positions as we iterate through the time steps sequentially. Doing so speeds up the computation significantly, however the parameters are no longer optimized properly. By utilizing these intermediary latent positions, the resulting positions are no longer directly related to the optimizable initial positions, thus making the computation graph incomplete.
    
    \subsubsection{Pre-computing Latent Spaces}
        
        An alternative is to compute all latent positions for all time steps beforehand, and simply pass the correct latent space representation to the appropriate step of the optimization procedure, thus keeping the computation graph intact. This is possible by specifying that the computation graph should be retained between backward passes. 
        As a result, the computation graph is still quite large for higher-order autoregressive models, but more importantly, doing so makes the optimizer calculate subsequent costs using positions computed prior to updating the parameters. Making small changes to the AR coefficients ($\phi$) in the autoregressive model has a huge impact on the latent space positions at later time steps, such that the pre-computed latent positions at time $t$ are entirely different from the actual latent positions at time $t$. Hence, there is no obvious way to avoid recomputing the latent positions after each optimization step.
        
    \subsubsection{Direct Computation}
    
        Fortunately, the most time-consuming part of optimizing is not the computation of the positions, but the traversal of the computation graph when calculating the gradients in the backward pass. As a result, the most efficient method of computation is by directly relating the latent positions at time $t$ to the adjustable initial positions. 
        This is trivial in terms of the static latent space model, where $Z_t=Z_0$, however the diffusion model models the latent positions at time $t$ based on the positions at time $t-1$, namely $Z_t=Z_{t-1}+\epsilon_t$.
        
        This corresponds to simply adding diffusion $\epsilon_t$ at each time step $t$, such that an equivalent representation is to model the latent positions as $Z_t=Z_0+\sum_{i=1}^t\epsilon_i$. Doing so, the computation graph comprises a constant number of operations opposed to growing linearly with $t$.
        For the general AR($p$) process on the other hand, where $Z_t=\sum_{i=1}^p \phi_1 Z_{t-i} + \epsilon_t$, there is no obvious way of computing $Z_t$ directly with respect to the initial positions $Z_{0\dots p-1}$. One alternative is to compute the polynomial of $Z_t$, however the number of terms grows exponentially with $t$ for $p>1$. To illustrate, the polynomial for the first three autoregressions of an AR(3) process is shown in \Cref{eq:pyimpl-ar-polynomial}.
        \begin{equation}\label{eq:pyimpl-ar-polynomial}
        \begin{split}
            Z_3 &= \phi_1Z_2 + \phi_2Z_1 + \phi_3Z_0 + \epsilon_3 \\
            Z_4 &= \phi_1^2Z_2 + \phi_1\phi_2Z_1 + \phi_1\phi_3Z_0 + \phi_1\epsilon_3 + \phi_2Z_2 + \phi_3Z_1 + \epsilon_4 \\
            Z_5 &= \phi_1^3Z_2 + \phi_1^2\phi_2Z_1 + \phi_1^2\phi_3Z_0 + \phi_1^2\epsilon_3 + 2\phi_1\phi_2Z_2 \\
                &+ \phi_1\phi_3Z_1 + \phi_2^2Z_1 + \phi_2\phi_3Z_0 + \phi_1\epsilon_4 + \phi_2\epsilon_3 + \phi_3Z_2 + \epsilon_5
        \end{split}
        \end{equation}
        Hence, the autoregressions are computed iteratively, with the size of the computation graph growing linearly with $t$.
        
    \subsubsection{Block Optimization}
    
        Nevertheless, the size of the computation graph can easily be constrained by limiting the number of future time steps to compute. Decreasing the amount of training data is not desirable, such that a more fitting solution is to divide the training data into several overlapping blocks. 
        The intuition is, that the first set of initial latent positions can be used to learn a second set of positions, being the initial positions for the second block. Continuing in this fashion, what is learned in each block is carried over to the next block, such that the total number of time steps can be increased without affecting the computation time of each individual block. 
        While the computational complexity becomes linear in the number of blocks instead of time steps, the time between each optimization step decreases significantly, thus converging much faster overall.
        
        In practice however, the information from the previous block is not carried over to the next, but simply discarded as the optimizer immediately starts overfitting to the limited time span. The result is a static representation of the latent positions at each block, since it is much easier to achieve a low loss by moving the block's initial positions than by adjusting the AR-coefficients and actually learning the temporal aspect.
        
    \subsubsection{Warm Start}
    
        Even though block optimization is not particularly successful, a similar approach can be used to give the optimizer a warm start, thus drastically reducing the time it takes to converge. Using only a single set of initial positions, the idea is to start by fitting to the first few time steps, and double the number of time steps to optimize until the whole training set is employed. Doing so, a high AUC score is quickly achieved for the first few time steps, and when the number of time steps is extended, the optimizer immediately adjusts the AR-coefficients to ensure a high AUC score for the following time steps as well. As a result, the optimizer is almost fully converged when training on the full dataset.
    
\subsection{Estimating Innovations}

    In time series analysis, the \emph{innovation} is the difference between the observed value of a variable at time $t$, and the optimal forecast of that value based on information available prior to time $t$. The innovations of a diffusion process or autoregressive process are often referred to as white noise, being a sequence of uncorrelated random variables with zero mean and finite variance. The signal is most often assumed to follow a Gaussian distribution given by
    \begin{equation}
        f(x\vert\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
    \end{equation}
    resulting in the following $n$ sample log-likelihood function
    \begin{equation}
        \ln\mathcal{L}(\mu, \sigma^2\vert x) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2
    \end{equation}
    
    Considering the sparsity of real-life networks, another fitting assumption is that the changes are less frequent, but have greater impact. This relates to whole groups or communities being connected or separated as a result of events such as introducing 
    

    The Laplace distribution is given by
    \begin{equation}
        f(x\vert\mu,\sigma)=\frac{1}{\sqrt{2}\sigma}e^{-\frac{\sqrt{2}|x-\mu|}{\sigma}}
    \end{equation}
    resulting in the following $n$ sample log-likelihood function
    \begin{equation}
        \ln\mathcal{L}(\mu,\sigma\vert x)=-n\ln(\sqrt{2}\sigma) -\frac{\sqrt{2}}{\sigma}\sum_{t=1}^n |x_i-\mu|
    \end{equation}
    
    
    \subsubsection{Sampling Prior Distribution}
    
        % One way of handling errors 
        The simple approach to error handling
    
    \subsubsection{Infer Noise Variable}
    
\subsection{Node Batching}

    Node batching is the procedure of selecting a subset of nodes for which to compute the log-likelihood and update the model. Many real-life networks contain thousands if not millions of nodes, such that processing the entire network on a GPU at once is infeasible. However, even for smaller networks it is desirable to perform batching, such that the log-likelihood is computed faster and the time between model updates is reduced. Optimizing with a batch size of one is referred to as stochastic training, where a single sample is randomly selected and used to update the model. Using a very small part of the dataset introduces noise in system, as the difference from batch to batch is much larger. While too much noise slows down the optimization procedure, noise is beneficial in terms of escaping local minima and saddle points. Hence, the optimal batch size is large enough to result in progressive updates, while small enough to update quickly and benefit from the noise on a plateau. The optimum differs for each dataset and should preferably be cross-validated.
    
    \subsubsection{The Batching Problem}
    
        There are two key components in a successful batching strategy, namely making sure all pairs of nodes are compared and that the negative log-likelihood loss of looping nodes ($i=j$) is zero. 
        Comparing all pairs of nodes is necessary to learn the whole representation of the underlying network, where $\sub(y)$ is not necessarily equal to $\sub(y)(ji)$ for directed networks. 
        An entity is never linked to itself, such that the diagonal of the sociomatrix $Y$ is always empty. As previously stated, the link probability $\sub(p)$ is inversely proportional to the distance between two nodes $i$ and $j$. However, when $i=j$, the distance from the node to itself is zero, which should coincide with a high link probability. To remedy this, the loss should be removed when comparing a node with itself.
        
    \subsubsection{Indexing}
    
        \citeauthor{jacobsen2018a} devised a batching strategy in \cite{jacobsen2018a}, where a set of $n$ numbers are randomly drawn from a uniform distribution with replacement. These numbers are then used as both row and column indices to extract an $n\times n$ submatrix $y$ from the much larger $N\times N$ sociomatrix $Y$. An example with an $8\times 8$ sociomatrix and the indices $\{1,4,6\}$ is shown in \Cref{BatchIndex}. Each index corresponds to a node with a given latent positions, which are used to compute the link probabilities for all pairs, resulting in an $n\times n$ matrix $\hat{y}$. Finally, the negative log-likelihood loss is calculated based on $y$ and $\hat{y}$. Since the row indices and the column indices are equal, the pairs for which $i=j$ are guaranteed to lie on the main diagonal of the loss matrix. Hence, correct behavior is ensured by simply removing the entries on the main diagonal before summarizing the total loss. 
        \xeq{\xtex/equations/{BatchIndex}}
        
        This strategy is easy to implement, making sure all combinations of nodes are compared over time by random sampling indices. By using the same set of indices to index the sociomatrix along both dimensions, the placement of the looping $i=j$ pairs is fixed, thus allowing for quick removal of the desired loss entries.
        An immediate improvement is to sample without replacement, avoiding duplicate indices in batches and ensuring that all nodes are sampled each epoch. However, a much more significant improvement is to represent the sociomatrix $Y$ sparsely.
        
    \subsubsection{Sparse Sociomatrix}
    
        A sparse matrix stores only non-zero elements, which is quite convenient considering the sparsity of real-life networks. To illustrate, one of the datasets considered has a total of $N=1899$ unique nodes spanning $T=193$ days. The total number of elements in the sociomatrix is thus over half a billion ($N^2T \approx 696\cdot 10^6$), whereas the total number of non-zero elements is barely 34,000. The motivation for using a sparse sociomatrix is that 34,000 elements can easily be stored on the GPU, while reserving space for other computations.
        
        The only available sparse representation of a PyTorch tensor is the COO-format. COO is an abbreviation of coordinate list, where each non-zero value is associated with a pair of coordinates. Considering the previously used example sociomatrix in \Cref{BatchIndex}, the following information is stored:
        \xtab{\xtex{SparseContents}}
        
        Hence, creating the submatrix $y$ using \citeauthor{jacobsen2018a}'s batching strategy requires looking up each individual pair of indices and inserting 1s and 0s depending on the contents of the sparse sociomatrix.
    
    \subsubsection{Slicing}
    
        This thesis proposes a new batching strategy which allows efficient sampling from a sparse sociomatrix by constraining the batches to slices. The first consideration is whether slices can be used to compare all combinations of nodes, also for directed networks.
        This is achieved by using different slices for rows and columns, such that a row slice can be compared with any column indices and vice versa. An example is shown in \Cref{BatchSlice} using the same sociomatrix as before, with the slices $\{(0,3), (5,8)\}$ not including the end to adhere to Python semantics.
        \xeq{\xtex/equations/{BatchSlice}}
        
        To ensure that all nodes appear equally frequently in the slices, the slices are created for the whole range of nodes and shuffled for each epoch to simulate uniform sampling without replacement.
        One issue is that the diagonal of looping nodes (where $i=j$) is no longer fixed, and in this particular example, there are no such elements at all. Not comparing looping nodes is beneficial, as they do not contribute to the log-likelihood. However, they cannot be excluded entirely, as a sociomatrix including the pairs $\{(0,1), (1,0)\}$ also include $\{(0,0), (1,1)\}$. Similarly, the log-likelihood loss should also be removed for looping nodes in overlapping slices.
        While the diagonal of looping nodes is no longer fixed, it can easily be found as the degree of overlap. More specifically, the placement of the diagonal is calculated as the start of the row slice minus the start of the column slice, where 0 is the main diagonal, a negative number is below the main diagonal and positive is above. Hence, this strategy also ensures that the loss is removed when comparing a node to itself.
        
        The question that remains, is why exactly slices. Considering the sparse representation of the sociomatrix in \Cref{SparseContents}, the indices can easily be filtered by lower and upper bounds. By conveniently using the row and column slices as bounds on the row and column indices respectively, the result is combined by a logical \texttt{AND} operation. The combined row and column mask is then used to index the original arrays of row and column indices, getting each pair of coordinates for each non-zero value within the slices. Performing these operations on the example sparse sociomatrix with the same slices as before $\{(0,3), (5,8)\}$ is illustrated in \Cref{SparseMask}. The coloring represents the resulting bit masks after filtering, with highlighted areas as 1s and grayed out areas as 0s. The highlighted values are the result from combining row and column masks, indicating non-zero values at indices $\{(0,6),(1,6),(2,5) \text{ and } (2,7)\}$ as expected.
        \xtab{\xtex{SparseMask}}
        
        The found indices are used to construct the smaller dense submatrix $y$, which is subsequently used to compute the negative log-likelihood loss as normal.
        Instead of constructing a dense matrix, an alternative approach is to construct a sparse matrix, and use the indices as a mask. PyTorch defines an operation \texttt{sparse\_mask} which selects values from a dense matrix based on the indices of a sparse matrix. The result of both approaches are equivalent, however the masking operation does not scale as drastically as the dense matrix multiplication as shown in \Cref{Method/Batch-Slicing-Mask}.
        \xfig{\ximg[width=0.7\textwidth]{Method/Batch-Slicing-Mask}}
        However, PyTorch has not yet implemented the operation's derivative, such that it cannot currently be used for model optimization. Regardless, doing so would not result in any immediate performance gain, as the batch sizes used in this project is well below a thousand.
        
% \subsection{Operation Profiling}

%     The operations are profiled on an RTX-2080 GPU running seven times with 1.000-100.000 loops depending on the run-time of the operation, showing timings as \verb|mean Â± std|.

%     \subsubsection{Indexing}
        
%         \begin{verbatim}
% %timeit E[:,idx]
% %timeit E.index_select(1,LongTensor(idx))
% %timeit FloatTensor(1,len(idx),2)
%         \end{verbatim}
%         \begin{verbatim}
% 642 Âµs Â± 5.82 Âµs
% 72.5 Âµs Â± 987 ns
% 2.3 Âµs Â± 19.4 ns
%         \end{verbatim}
        
        
%         \begin{verbatim}
% %timeit E[:,idx].normal_() * E_std
% %timeit E.index_select(1,LongTensor(idx)).normal_() * E_std
% %timeit FloatTensor(1,len(idx),2).normal_() * E_std
% %timeit torch.normal(FloatTensor(1,len(idx),2).fill_(0), \
%                         E_std.expand((1,len(idx),2)))
% mean = FloatTensor(1,len(idx),2).fill_(0)
% std = E_std.expand((1,len(idx),2))
% %timeit torch.normal(mean, std)
%         \end{verbatim}
%         \begin{verbatim}
% 746 Âµs Â± 11.8 Âµs
% 163 Âµs Â± 2.47 Âµs
% 108 Âµs Â± 1.39 Âµs
% 85.6 Âµs Â± 1.64 Âµs
% 27.7 Âµs Â± 490 ns
%         \end{verbatim}
        
    
% Som jeg forstÃ¥r pÃ¥ den kode jeg sÃ¥ i forbindelse med vores Skype mÃ¸de sampler du fra  (prior) fordelingen stÃ¸jen, i.e.


% $e_t\sim N(0,diag(\sigma^2_e))$

% og lÃ¦rer hvad $\sigma^2_e$ er som del af din inferens.


% Dette svarer til i modellen

% $p(A,e|\theta,z)= p(A|\theta,z,e)p(e|\sigma^2_e) $
% hvor A er dit netvÃ¦rk, theta dine AR parametre, z de latente positioner og e stÃ¸j innovationerne, at du forsÃ¸ger at marginalisere e ved at approksimerer denne marginalisering med et enkelt sample fra $p(e|\sigma^2_e)$ , i.e.


% $p(A|\theta,z)=\int p(A|\theta,z,e)p(e|\sigma^2_e)  de \approx p(A|\theta,z,e_s)$

% hvor $e_s\sim p(e|\sigma^2_e)$. 



% Dette er som jeg ser det helt OK, men en anden mulighed var at forsÃ¸ge at inferere ogsÃ¥ E, i.e. minimere:

% $-log(p(A,e|\theta,z))=-log( p(A|\theta,z,e)p(e|\sigma^2_e) )=-log( p(A|\theta,z,e))-log(p(e|\sigma^2_e) )$

% Hvor det fÃ¸rste led er krydsentropien og det andet led den associerede omkostning ved at skulle tilfÃ¸je stÃ¸j. Ved ikke om dette potentielt kan vÃ¦re en fordel i forhold til inferensen. Du kunne overveje at undersÃ¸ge begge muligheder.



% NÃ¥r du prÃ¦diktere skal du stadig sample e som du hidtil har gjort fra $p(e|\sigma^2_e)$.