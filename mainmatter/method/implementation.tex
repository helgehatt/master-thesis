\section{PyTorch Implementation}

The models and the MLE framework are implemented in PyTorch, an open-source deep learning platform aiming to provide a seamless path from research prototyping to production deployment. % PyTorch version

\subsection{Pairwise Distance}

    Calculating link probabilities for a subset of nodes requires the computation of the pairwise distance matrix $\D$, where $(\sub(d))^2=\Vert \z_i-\z_j\Vert^2$ is the squared euclidean distance between nodes $i$ and $j$. The squared euclidean distance differs from euclidean distance by omitting the square root for faster clustering. The distance matrix is symmetric for an undirected latent space, while a directed latent space has separate representations for incoming and outgoing edges. In the latter case, $\D$ is calculated from the latent space positions $\Z_{in}$ and $\Z_{out}$ such that $\sub(d)\neq\sub(d)(ji)$.
    
    Considering two arbitrary matrices $\A$ and $\B$ of size $N\times k$, the direct approach to computing the distance matrix is by adding extra dimensions such that the size of $\A$ is $N\times 1\times k$ and $\B$ is $1\times N\times k$. The squared differences between all combinations of rows and columns are then computed simultaneously as $(\A-\B)^2$, resulting in a $N\times N\times k$ matrix. The final step is to sum over the last dimension, giving rise to the $N\times N$ distance matrix. This approach is efficient for very small matrices, however scales poorly both in terms of memory consumption and time complexity.
    
    A less memory intensive approach is to calculate the squared norm of the difference using quadratic expansion, shown in \Cref{eq:pyimpl-dist-quadratic}.
    \begin{equation}\label{eq:pyimpl-dist-quadratic}
        (\sub(d))^2=\Vert\z_i-\z_j\Vert^2\equiv\Vert \z_i\Vert^2 + \Vert\z_j\Vert^2 - 2(\z_i\cdot\z_j)
    \end{equation}
    The squared norm of a matrix $\A$ is computed by squaring all $N\times k$ entries and summing over the last dimension, resulting in a vector of size $N$. Viewing $\Vert\A\Vert^2$ as a column vector and $\Vert\B\Vert^2$ as a row vector, the full $N\times N$ distance matrix is computed as $\Vert\A\Vert^2 + \Vert \B\Vert^2 - 2\A\B^T$. Doing so, the memory consumption is $O(N^2)$ for the final distance matrix (assuming $k\ll N$), down from $O(N^2k)$ for the intermediary results of the direct approach.
    
    To illustrate the difference in computational complexity between the two methods, \Cref{Method/Pairwise-Distance-N} shows the time it takes to compute the calculations (in Âµs) as a function of $N$ and as a function of $k$ in \Cref{Method/Pairwise-Distance-k}. The timings are the mean of 7 runs with 100-1000 loops each, carried out on an RTX-2080 GPU. Both the direct approach and the approach utilizing quadratic expansion scale exponentially with $N$, however the latter with a much lower rate. Additionally, the first method scales linearly with $k$, while the second remains constant. Hence, empirical tests show that using quadratic expansion is a much more efficient way of computing the pairwise distances as the number of nodes in a batch and the dimensionality of the latent space increases. These findings contradict \citeauthor{jacobsen2018a}'s argument in \cite{jacobsen2018a}, stating that the direct approach is computationally fastest, ``because it allows full GPU parallelism over the entire process of computing the pairwise distance matrix''.
    \xfig{\xdouble
        {\ximg{Method/Pairwise-Distance-N}}
        {\ximg{Method/Pairwise-Distance-k}}
    }
    
\subsection{Latent Space Positions}\label{sec:pyimpl-positions}

    One of the most time-consuming tasks is the computation of the latent space positions, particularly in terms of the autoregressive process where the latent positions at time $t$ depend on all $t-1$ previous latent spaces.
    There are two key challenges to keep in mind, namely keeping the PyTorch computation graph intact as well as keeping the optimization procedure well-behaved. 
    %This entails that each step of the optimizer contributes to a decrease in the overall loss, and that the gradients will eventually converge to zero.
    
    Starting with a subset of nodes ($batch$) at a particular time $t$, the predictions $\sub(p)\text{ for } i,j \in batch$ are computed, assigning high and low probabilities depending on the bias $\beta$, the latent positions $\Z_t$ and other model parameters. These predictions are compared to the true results, indicating which nodes are actually linked, resulting in a $loss$ which is inversely proportional to the correctness of the model. Every single step, from the batch is drawn to the loss is computed, is recorded in PyTorch's computation graph, documenting exactly which operations were performed to get from batch to result. 
    
    Now, to ensure better predictions at the next iteration, PyTorch propagates backward through the recorded graph, computing the gradients with respect to the latent positions and the other parameters. Doing so, the optimizer knows which parameters to increase and decrease, including which direction to move the latent positions.
    The caveat is, that if the chain of operations is broken, the optimizer will have no way of computing the gradients with respect to the adjustable parameters. 
    
    \subsubsection{Storing Intermediate Positions}
        
        The most obvious solution is to simply recompute everything for each batch and for each time step. Doing so is however quite redundant considering the autoregressive model has to compute the latent positions for all time steps $0\leq t'<t$ before computing the positions at time $t$. Hence, it is tempting to store the computed latent space positions as we iterate through the time steps sequentially. Doing so speeds up the computation significantly, however the parameters are no longer optimized properly. By utilizing these intermediary latent positions, the resulting positions are no longer directly related to the optimizable initial positions, thus making the computation graph incomplete.
    
    \subsubsection{Pre-computing Latent Spaces}
        
        An alternative is to compute all latent positions for all time steps beforehand, and simply pass the correct latent space representation to the appropriate step of the optimization procedure, thus keeping the computation graph intact. This is possible by specifying that the computation graph should be retained between backward passes. 
        As a result, the computation graph is still quite large for higher-order autoregressive models, but more importantly, doing so makes the optimizer calculate subsequent costs using positions computed prior to updating the parameters. Making small changes to the AR coefficients ($\bmphi$) in the autoregressive model has a huge impact on the latent space positions at later time steps, such that the pre-computed latent positions at time $t$ are entirely different from the actual latent positions at time $t$. Hence, there is no obvious way to avoid recomputing the latent positions after each optimization step.
        
    \subsubsection{Direct Computation}
    
        Fortunately, the most time-consuming part of optimizing is not the computation of the positions, but the traversal of the computation graph when calculating the gradients in the backward pass. As a result, the most efficient method of computation is by directly relating the latent positions at time $t$ to the adjustable initial positions. 
        This is trivial in terms of the static latent space model, where $\Z_t=\Z_0$, however the diffusion model models the latent positions at time $t$ based on the positions at time $t-1$, namely $\Z_t=\Z_{t-1}+\bmepsilon_t$.
        
        This corresponds to simply adding diffusion $\bmepsilon_t$ at each time step $t$, such that an equivalent representation is to model the latent positions as $\Z_t=\Z_0+\sum_{i=1}^t\bmepsilon_i$. Doing so, the computation graph comprises a constant number of operations opposed to growing linearly with $t$.
        For the general AR($p$) process on the other hand, where $\Z_t=\sum_{i=1}^p \bmphi_1 \Z_{t-i} + \bmepsilon_t$, there is no obvious way of computing $\Z_t$ directly with respect to the initial positions $\Z_{0\dots p-1}$. One alternative is to compute the polynomial of $\Z_t$, however the number of terms grows exponentially with $t$ for $p>1$. To illustrate this, the polynomial for the first three autoregressions of an AR(3) process is shown in \Cref{eq:pyimpl-ar-polynomial}.
        \begin{equation}\label{eq:pyimpl-ar-polynomial}
        \begin{split}
            \Z_3 &= \bmphi_1\Z_2 + \bmphi_2\Z_1 + \bmphi_3\Z_0 + \bmepsilon_3 \\
            \Z_4 &= \bmphi_1^2\Z_2 + \bmphi_1\bmphi_2\Z_1 + \bmphi_1\bmphi_3\Z_0 + \bmphi_1\bmepsilon_3 + \bmphi_2\Z_2 + \bmphi_3\Z_1 + \bmepsilon_4 \\
            \Z_5 &= \bmphi_1^3\Z_2 + \bmphi_1^2\bmphi_2\Z_1 + \bmphi_1^2\bmphi_3\Z_0 + \bmphi_1^2\bmepsilon_3 + 2\bmphi_1\bmphi_2\Z_2 \\
                &+ \bmphi_1\bmphi_3\Z_1 + \bmphi_2^2\Z_1 + \bmphi_2\bmphi_3\Z_0 + \bmphi_1\bmepsilon_4 + \bmphi_2\bmepsilon_3 + \bmphi_3\Z_2 + \bmepsilon_5
        \end{split}
        \end{equation}
        Hence, the autoregressions are computed iteratively, with the size of the computation graph growing linearly with $t$.
        
    \subsubsection{Block Optimization}
    
        Nevertheless, the size of the computation graph can easily be constrained by limiting the number of future time steps to compute. Decreasing the amount of training data is not desirable, therefore, a more fitting solution is to divide the training data into several overlapping blocks. 
        The intuition is, that the first set of latent positions can be used to learn a second set of positions. Continuing in this fashion, what is learned in each block is carried over to the next block, such that the total number of time steps can be increased without affecting the computation time of each individual block. 
        While the computational complexity becomes linear in the number of blocks instead of time steps, the time between each optimization step decreases significantly, thus converging much faster overall.
        
        In practice however, the information from the previous block is not carried over to the next, but simply discarded as the optimizer immediately starts overfitting to the limited time span. The result is a static representation of the latent positions in each block, since it is much easier to achieve a low loss by moving the block's initial positions than by adjusting the AR-coefficients and actually learning the temporal aspect.
        
    \subsubsection{Warm Start}
    
        Even though block optimization is not particularly successful, a similar approach can be used to give the optimizer a warm start, thus drastically reducing the time it takes to converge. Using only a single set of initial positions, the idea is to start by fitting to the first few time steps, and double the number of time steps to optimize until the whole training set is employed. Doing so, a high AUC score is quickly achieved for the first few time steps, and when the number of time steps is extended, the optimizer immediately adjusts the AR-coefficients to ensure a high AUC score for the following time steps as well. As a result, the optimizer is almost fully converged when training on the full dataset.
    
\subsection{Estimating Innovations}\label{sec:pyimpl-innovations}

    In time series analysis, the \emph{innovation} is the difference between the observed value of a variable at time $t$, and the optimal forecast of that value based on information available prior to time $t$. The innovations of a diffusion process or autoregressive process are often referred to as white noise, being a sequence of uncorrelated random variables with zero mean and finite variance. The signal is most often assumed to follow a Gaussian distribution given by
    \begin{equation}
        f(\x\vert\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(\x-\mu)^2}{2\sigma^2}}
    \end{equation}
    
    Considering the sparsity of real-life networks, another fitting assumption is that the changes are less frequent, but have bigger impact. This relates to whole groups or communities being connected or separated as a result of various events. This is modeled by the Laplace distribution, being shaped as two exponential distributions facing each other, given by
    \begin{equation}
        f(\x\vert\mu,\sigma)=\frac{1}{\sqrt{2}\sigma}e^{-\frac{\sqrt{2}|\x-\mu|}{\sigma}}
    \end{equation}
    
    \subsubsection{Sampling Prior Distribution}
    
        The simple approach to error handling is by sampling the errors from its prior distribution $\bmepsilon_t \sim N(\bm{0},\bmsigma_\epsilon^2)$ and learn $\bmsigma_\epsilon^2$ as a part of the maximum likelihood inference.
        This corresponds to the model
        \begin{equation}
            P(\Y_t,\bmepsilon|\Z_t,\theta) = P(\Y_t|\Z_t,\bmtheta,\bmepsilon)P(\bmepsilon|\bmsigma_\epsilon^2)
        \end{equation}
        with $\Y$ as the network, $\Z$ the latent positions, $\bmtheta$ the model parameters (e.g. bias and AR-coefficients) and $\bmepsilon$ the innovations, where $\bmepsilon$ is marginalized by approximating the marginalization with a single sample $\bmepsilon_s \sim P(\bmepsilon|\bmsigma_\epsilon^2)$, such that
        \begin{equation}
            P(\Y_t|\Z_t,\bmtheta) = \int P(\Y_t|\Z_t,\bmtheta,\bmepsilon) P(\bmepsilon|\bmsigma_\epsilon^2) d\bmepsilon \approx P(\Y_t|\Z_t,\bmtheta,\bmepsilon_s)
        \end{equation}
        The benefit of this simple approach, is that the number of model parameters is kept to a minimum, estimating only the $k$-dimensional vector of variances $\bmsigma_\epsilon^2$. However, doing so has a huge negative impact, as the sampled innovations $\bmepsilon_s$ are completely different from epoch to epoch. Hence, the cumulative errors introduce a regularizing effect, making the model incapable of learning the temporal aspect. This issue is visualized in \Cref{Method/Diffusion-Over-Time}, where the drawn sample results in a predicted position $p_1$, entirely different from the underlying innovation leading to the true position at $t_1$. 
        \xfig{\ximg{Method/Diffusion-Over-Time}[Example of true and predicted diffusion sampling from the prior distribution]}
        Continuing from $p_1$, predicting $t_2$ is not even possible with the current variance, and as time progresses the difference between predicted and true latent positions will keep increasing.
    
    \subsubsection{Inferring Innovations Directly}

        An alternative approach is to infer the innovations $\bmepsilon$ directly during optimization, such that the model no longer samples blindly, but adjusts the error to better capture the underlying movement. 
        Doing so, it is important to penalize the addition of noise to prevent overfitting and incentivize the learning of other model parameters. The innovations for test data predictions are still sampled from $P(\bmepsilon|\bmsigma_\epsilon^2)$, such that allowing the innovations to fit perfectly to the training data does not generalize well to test data. Hence, a more fitting solution is to minimize the negative log-likelihood loss associated with $\bmepsilon$. The $n$ sample log-likelihood function for a Gaussian distribution is given by
        \begin{equation}\label{error-gaussian-log-loss}
            \log\mathcal{L}(\mu, \sigma^2\vert \x) = -\frac{n}{2}\log2\pi - \frac{n}{2}\log\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2
        \end{equation}
        and for a Laplace distribution by
        \begin{equation}\label{error-laplace-log-loss}
            \log\mathcal{L}(\mu,\sigma\vert \x) = -n\log\sqrt{2}\sigma -\frac{\sqrt{2}} {\sigma} \sum_{t=1}^n |x_i-\mu|
        \end{equation}
    
        Depending on the assumption of the errors' distribution, the log-likelihood functions can be used to reduce the amount of diffusion added and learn $\bmsigma_\epsilon^2$ as a counterweight to the size of the innovations. The disadvantage of this approach, is that the number of parameters to estimate increases by $d\cdot N\cdot k$ for each time step of the optimization procedure.
        
    \subsubsection{Analytically Optimized Variance}
        
        Instead of learning $\bmsigma_\epsilon^2$ as a part of the model inference, the variance can be chosen to maximize the log-likelihood function as shown in \cite{haerdle2015statistics}. The MLE of $\sigma^2$ for the Gaussian distribution is found by taking the partial derivative with respect to $\sigma^2$ and setting the resulting equation equal to zero. Considering that the Gaussian models noise, $\mu$ is assumed to be zero, resulting in
        % \begin{equation}\label{error-mu-partial}
        %     \frac{\partial\log\mathcal{L}}{\partial\mu} = -\frac{2}{2\sigma^2}\sum_{i=1}^n(x_i-\hat{\mu})\cdot(-1) = 0
        % \end{equation}
        \begin{equation}\label{error-sigma-partial}
            \frac{\partial\log\mathcal{L}}{\partial\sigma^2} = -\frac{n}{2} \cdot \frac{1}{\hat{\sigma}^2} + \frac{1}{2} \cdot \frac{1}{\hat{\sigma}^4} \cdot \sum_{i=1}^n x_i^2 = 0
        \end{equation}
        % \Cref{error-mu-partial} is solved to produce the ML estimator $\hat{\mu}$ of $\mu$ as follows
        % \begin{equation}
        %     \sum_{i=1}^n(x_i-\hat{\mu}) = 0 \equiv \hat{\mu}=\frac{1}{n}\sum_{i=1}^n x_i = \bar{x}
        % \end{equation}
        % such that substituting the result in \Cref{error-sigma-partial} allows solving for $\hat{\sigma}^2$
        which is solved for the sample variance $\hat{\sigma}^2$ as follows
        \begin{equation}
            \frac{n}{2\hat{\sigma}^2} = \frac{1}{2\hat{\sigma}^4} \sum_{i=1}^n x_i^2 \equiv \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n x_i^2
        \end{equation}
        
        Inserting the sample variance into the log-likelihood function results in the following log-likelihood, removing the variance as a model parameter.
        \begin{equation}
            \log\mathcal{L}(\x)=-\frac{n}{2}\log 2\pi -\frac{n}{2}\log\sum_{i=1}^n x_i^2-\frac{n}{2}\log n-\frac{n}{2}
            % \frac{1}{2}\sum_i^n \log\left(\frac{2\pi}{n}\sum_i^n \epsilon_i^2\right) + \frac{N}{2}
        \end{equation}
        Maximizing this log-likelihood opposed to the original Gaussian log-likelihood ensures that the innovations are optimized towards an optimal $\bmsigma_\epsilon^2$.
    
\subsection{Node Batching}

    Node batching is the procedure of selecting a subset of nodes for which to compute the log-likelihood and update the model. Many real-life networks contain thousands if not millions of nodes, such that processing the entire network on a GPU at once is not feasible. However, even for smaller networks it is desirable to perform batching, such that the log-likelihood is computed faster and the time between model updates is reduced. Optimizing with a batch size of one is referred to as stochastic training, where a single sample is randomly selected and used to update the model. Using a very small part of the dataset introduces noise in system, as the difference from batch to batch is much larger. While too much noise slows down the optimization procedure, noise is beneficial in terms of escaping local minima and saddle points. Hence, an optimal batch size is large enough for progressive updates, while small enough to update quickly and benefit from the noise on a plateau. The optimum differs for each dataset and should preferably be cross-validated.
    
    \subsubsection{The Batching Problem}
    
        There are two key components in a successful batching strategy, namely making sure all pairs of nodes are compared and that the negative log-likelihood loss of looping nodes ($i=j$) is zero. 
        Comparing all pairs of nodes is necessary to learn the whole representation of the underlying network, where $\sub(y)$ is not necessarily equal to $\sub(y)(ji)$ for directed networks. 
        An entity is never linked to itself, such that the diagonal of the sociomatrix $\Y$ is always empty. As previously stated, the link probability $\sub(p)$ is inversely proportional to the distance between two nodes $i$ and $j$. However, when $i=j$, the distance from the node to itself is zero, which should coincide with a high link probability. To remedy this, the loss should be removed when comparing a node with itself.
        
    \subsubsection{Indexing}
    
        \citeauthor{jacobsen2018a} devised a batching strategy in \cite{jacobsen2018a}, where a set of $n$ numbers are randomly drawn from a uniform distribution with replacement. These numbers are then used as both row and column indices to extract an $n\times n$ submatrix $\Y_{sub}$ from the much larger $N\times N$ sociomatrix $\Y$. Note that \citeauthor{jacobsen2018a} only considered static networks, such that $\Y$ does not include the additional time dimension.
        An example with an $8\times 8$ sociomatrix and the indices $\{1,4,6\}$ is shown in \Cref{BatchIndex}. Each index corresponds to a node with a given latent position, which is used to compute the link probabilities for all pairs, resulting in an $n\times n$ matrix $\P_{sub}$. Finally, the negative log-likelihood loss is calculated based on $\Y_{sub}$ and $\P_{sub}$. Since the row indices and the column indices are equal, the pairs for which $i=j$ are guaranteed to lie on the main diagonal of the loss matrix. Hence, correct behavior is ensured by simply removing the entries on the main diagonal before summarizing the total loss. 
        \xeq{\xtex/equations/{BatchIndex}}
        
        This strategy is easy to implement, making sure all combinations of nodes are compared over time by random sampling indices. By using the same set of indices to index the sociomatrix along both dimensions, the placement of the looping $i=j$ pairs is fixed, thus allowing for quick removal of the desired loss entries.
        An immediate improvement is to sample without replacement, avoiding duplicate indices in batches and ensuring that all nodes are sampled each epoch. However, a much more significant improvement is to represent the sociomatrix $\Y$ sparsely.
        
    \subsubsection{Sparse Sociomatrix}
    
        A sparse matrix stores only non-zero elements, which is quite convenient considering the sparsity of real-life networks. To illustrate, one of the datasets considered has a total of $N=1899$ unique nodes spanning $T=193$ days. The total number of elements in the temporal sociomatrix $\tY$ is thus over half a billion ($N^2T \approx 696\cdot 10^6$), whereas the total number of non-zero elements is barely 34,000. The motivation for using a sparse sociomatrix is that 34,000 elements can easily be stored on the GPU, while reserving space for other computations.
        
        The only available sparse representation of a PyTorch tensor is the COO-format. COO is an abbreviation of coordinate list, where each non-zero value is associated with a pair of coordinates. Considering the previously used example sociomatrix in \Cref{BatchIndex}, the following information is stored:
        \xtab{\xtex{SparseContents}}
        
        Hence, creating the submatrix $\Y_{sub}$ using \citeauthor{jacobsen2018a}'s batching strategy requires looking up each individual pair of indices and inserting 1s and 0s depending on the contents of the sparse sociomatrix.
    
    \subsubsection{Slicing}
    
        This thesis proposes a new batching strategy which allows efficient sampling from a sparse sociomatrix by constraining the batches to slices. The first consideration is whether slices can be used to compare all combinations of nodes, also for directed networks.
        This is achieved by using different slices for rows and columns, such that a row slice can be compared with any column indices and vice versa. An example is shown in \Cref{BatchSlice} using the same sociomatrix as before, with the slices $\{(0,3), (5,8)\}$ not including the end to adhere to Python semantics.
        \xeq{\xtex/equations/{BatchSlice}}
        
        To ensure that all nodes appear equally frequently in the slices, the slices are created for the whole range of nodes and shuffled for each epoch to simulate uniform sampling without replacement.
        One issue is that the diagonal of looping nodes (where $i=j$) is no longer fixed, and in this particular example, there are no such elements at all. Not comparing looping nodes is beneficial, as they do not contribute to the log-likelihood. However, they cannot be excluded entirely, as a sociomatrix including the pairs $\{(0,1), (1,0)\}$ also include $\{(0,0), (1,1)\}$. Similarly, the log-likelihood loss should also be removed for looping nodes in overlapping slices.
        While the diagonal of looping nodes is no longer fixed, it can easily be found as the degree of overlap. More specifically, the placement of the diagonal is calculated as the start of the row slice minus the start of the column slice, where 0 is the main diagonal, a negative number is below the main diagonal and positive is above. Hence, this strategy also ensures that the loss is removed when comparing a node to itself.
        
        The question that remains, is why exactly slices. Considering the sparse representation of the sociomatrix in \Cref{SparseContents}, the indices can easily be filtered by lower and upper bounds. By conveniently using the row and column slices as bounds on the row and column indices respectively, the result is combined by a logical \texttt{AND} operation. The combined row and column mask is then used to index the original arrays of row and column indices, getting each pair of coordinates for each non-zero value within the slices. Performing these operations on the example sparse sociomatrix with the same slices as before $\{(0,3), (5,8)\}$ is illustrated in \Cref{SparseMask}. The coloring represents the resulting bit masks after filtering, with highlighted areas as 1s and grayed out areas as 0s. The highlighted values are the result from combining row and column masks, indicating non-zero values at indices $\{(0,6),(1,6),(2,5) \text{ and } (2,7)\}$ as expected.
        \xtab{\xtex{SparseMask}}
        
        The found indices are used to construct the smaller dense submatrix $\Y_{sub}$, which is subsequently used to compute the negative log-likelihood loss as normal.
        Instead of constructing a dense matrix, an alternative approach is to construct a sparse matrix, and use the indices as a mask. PyTorch defines an operation \texttt{sparse\_mask} which selects values from a dense matrix based on the indices of a sparse matrix. The result of both approaches are equivalent, however the masking operation does not scale as drastically as the dense matrix multiplication as shown in \Cref{Method/Batch-Slicing-Mask}.
        
        \xfig{\ximg[width=0.5\textwidth]{Method/Batch-Slicing-Mask}}
        However, the operation's derivative is not yet implemented in PyTorch, such that it cannot currently be used for model optimization. Regardless, doing so would not result in any immediate performance gain, as the batch sizes used in this project is well below the time complexity trade-off point.
        
\subsection{Sparse Validation}

    As previously mentioned in \Cref{sec:meval}, the performance of a model is evaluated in terms of its AUC score. During optimization, the model's performance is validated every ten epochs, being able to monitor the training progress. The validation score is used on several occasions, particularly as a stopping criteria, choosing when to stop training or increase the amount of training data. Moreover, the score is used to select which parameters to store, saving the model achieving the highest performance.
    
    Representing the $T\times N\times N$-dimensional sociomatrix $\tY$ sparsely allows efficient node batching by limiting data transfer between CPU and GPU. Doing so, networks of much larger size can be considered, as long as the sparse representation fits on the GPU. However, increasing the number of nodes to more than a thousand, another bottleneck emerges, namely validation. Validation is performed by computing individual AUC scores for each time step $t$, based on $\Y_t$ and $\Z_t$ for all $N$ nodes. Using the \href{https://scikit-learn.org/stable/}{\color{blue}{scikit-learn}} library to perform the actual calculations, validation suddenly requires a lot of data transfer.
    
    This is remedied by implementing a sparse validation technique, utilizing only a subset of nodes to acquire an estimate of the model's performance. While the idea is similar to node batching, there is one small subtlety to be aware of. Slicing blindly may result in a submatrix of $\Y_t$ containing only zeros (no links), for which the AUC is not defined. Hence, the slices are not chosen at random, but selected to ensure that there is at least one link present in the subset.
    A downside is that the sparsely computed AUC score does not necessarily accurately reflect the model's true AUC score, where a larger subset increases the accuracy, but also the computation time.
    