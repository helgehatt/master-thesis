\section{Model Development}

Three different types of models are considered, the first one being the static latent space model, shaping the foundation for the dynamic models.
Few additions are required to capture the temporal aspect with respect to diffusion, modeling the changes over time as small errors. The diffusion model is one of the simplest dynamic models, which is extended to an autoregressive model capable of identifying periodic patterns and more complex relations in temporal data.

\subsection{Latent Space Model}

    The latent space approach to social network analysis is introduced by \citeauthor{hoff2002latent} \cite{hoff2002latent}, using a model similar to Multidimensional Scaling in which entities are associated with locations in a $k$-dimensional space and where links are more likely if the entities are close in latent space.
    Given two entities $i$ and $j$, linkage is denoted by $\link$ and absence of a link by $\link*$, while $p(\link)$ or just $\sub(p)$ denotes the probability of observing the link. 
    Euclidean distance is used to measure the similarity between entities in the latent space, denoted as $\sub(d)$, however any distance metric satisfying the triangle inequality can be used.
    The latent space model is inherently reciprocal, where a small $\sub(d)$ follows from $\link$, making the link $\link(j)(i)$ more probable. Similarly, small $\sub(d)$ and $\sub(d)(jk)$ results in small $\sub(d)(ik)$, making the model inherently transitive as well.
    
    The method proposed in \cite{hoff2002latent} uses a conditional independence approach, assuming that the presence or absence of a link between two observations is independent of all other links in the network. Given the latent space positions $Z$, the covariate information $X$ and parameters $\theta$ the model is formalized in \Cref{eq:lsm-link-proba}. 
    
    \begin{equation}\label{eq:lsm-link-proba}
        P(Y|Z,X,\theta) = \prod_{i\neq j} P(\sub(y)|z_i,z_j,\sub(x),\theta)
    \end{equation}
    
    Covariate information includes pair-specific characteristics such as an indicator of whether two actors $i$ and $j$ are of the same sex. This was used by \citeauthor*{hoff2002latent} when analyzing strong friendship ties between boys and girls in a sixth-grade classroom. Providing such information can help increase model accuracy without adding much more complexity. However, the goal is to devise generalized models applicable to any type of network, such that covariate information is disregarded in the thesis. 
    
    Deciding whether links are present or absent can be considered a binary classification task, making the logistic regression model suitable to compute the odds of a link $\eta$ using the latent space positions $Z$ and a single parameter $\beta$ for the bias (intercept), as in \Cref{eq:lsm-link-odds}.
    
    \begin{equation}\label{eq:lsm-link-odds}
        \begin{split}
            \sub(\eta) &= \log \odds(\sub(y)=1|z_i,z_j,\beta) \\
                       &= \beta - |z_i-z_j|
        \end{split}
    \end{equation}
    
    To transform the odds to a probability measure, the sigmoid function is applied to $\eta$ in \Cref{eq:lsm-link-sigmoid}, restricting the output to the interval $[0,1]$.
    
    \begin{equation}\label{eq:lsm-link-sigmoid}
        \sub(p) = \sigma(\sub(\eta)) = \frac{1}{1 + e^{-(\beta - |z_i-z_j|)}}
    \end{equation}
    
    The link probabilities $\sub(p)$ is thus inverse proportional to the distance between the observations in the latent space, with a bias term to skew the probabilities in either direction.
    Having a conditional independence model, the log-likelihood is simply defined in \Cref{eq:lsm-log-likelihood}, which is used to estimate the model parameters using maximum likelihood techniques, or equivalently, by minimizing the negative log-likelihood function.
    
    \begin{equation}\label{eq:lsm-log-likelihood}
        \log P(Y|\eta) = \sum_{i\neq j}\{ \sub(\eta)\sub(y) - \log(1+e^{\sub(\eta)}) \}
    \end{equation}
    
    Note that the distances between a set of points in Euclidean space are invariant under rotation, reflection, and translation. Hence, there is an infinite number of latent space positions giving the same log-likelihood. The number of parameters to estimate depends on the number of entities $N$ in the network as well as the chosen dimensionality $k$ of the latent space, resulting in $N \times k + 1$ parameters. The $N\times k$ parameters corresponds to the latent space representation, while the $+1$ is due to the bias which is kept as a single scalar. \citeauthor{jacobsen2018a} experimented with specific biases for each row and column of the sociomatrix, corresponding to two vectors of dimensions $N\times 1$ and $1\times N$, though the results showed a single scalar to generalize better, allowing less wiggle room in the estimation of latent positions.
    
\subsection{Diffusion Model}
    
    A dynamic latent space model for social network analysis is presented by \citeauthor{sarkar2005dynamic} in \cite{sarkar2005dynamic} using a standard Markov assumption, i.e. that the latent locations at time $t+1$ are conditionally independent of all previous locations given the latent locations at time $t$. The solution is comprised of two parts, an observation model and a transition model. As before, $\sub(d)=|z_i-z_j|$ is the Euclidean distance between observations in the latent space at a given time $t$.
    
    The observation model is similar to the latent space model, however with two important alterations. Firstly, each entity is associated with a radius $r_i$, allowing entities to vary their sociability by limiting their sphere of interaction within the latent space.
    Secondly, the link probability is weighed by a kernel function to ensure a high probability of linkage if latent positions are within $\sub(r)=max(r_i,r_j)$ of one another, and a constant noise probability $\epsilon$ otherwise.
    
    For optimization purposes, the kernelized function must be continuous and differentiable at $\sub(d)=\sub(r)$, hence choosing the biquadratic kernel as follows
    \begin{equation}\label{eq:biquadratic-kernel}
        K(\sub(d))=
        \begin{cases}
            (1-(\sub(d)/\sub(r))^2)^2 & \text{when } \sub(d) \leq \sub(r) \\
            \ 0 & \text{otherwise}
        \end{cases}
    \end{equation}
    resulting in the link probability shown in \Cref{eq:dsnl-link-proba}.
    
    \begin{equation}\label{eq:dsnl-link-proba}
        \sub(p)=\frac{1}{1 + e^{-(\sub(r)-\sub(d))}} K(\sub(d)) + \epsilon(1-K(\sub(d)))
    \end{equation}
    
    The second part, namely the transition model, penalizes large displacements from the previous time step by having each coordinate of each latent position independently subjected to a Gaussian perturbation with zero mean and $\sigma^2$ variance. This results in the following Gaussian model
    \begin{equation}\label{eq:dsnl-transition}
        \log P(Z_t|Z_{t-1}) = -\sum_{i=1}^n |Z_{i,t}-Z_{i,t-1}|^2 / \sigma^2 + c
    \end{equation}
    where the goal is to optimize log-likelihood of the graphs $Y_{0..t}$ conditioned on the latent positions $Z_{0..t}$.
    
    Using these elements to account for the temporal aspect, a simple diffusion model is formalized in \Cref{eq:dsnl-model} by applying a standard Markov assumption. 
    
    \begin{equation}\label{eq:dsnl-model}
        P(Y_t|Z_t,\beta,\sigma_\epsilon) = \prod_{\link}\sub(p)\prod_{\link*}(1-\sub(p))
    \end{equation}
    
    The link probabilities $\sub(p)$ are calculated as before, however with the latent space at a given time step modeled as the previous latent space with added diffusion, as shown in \Cref{eq:dsnl-latent-space}. The diffusion follows a Gaussian with zero mean and $\sigma_\epsilon^2$ variance, known as the diffusion rate.
    
    \begin{equation}\label{eq:dsnl-latent-space}
        Z_t = Z_{t-1} + \epsilon_t \qquad\qquad \epsilon \sim N(0,\sigma_\epsilon^2)
    \end{equation}
    
    The diffusion rate determines how far observations can drift in a single time step, such that a fairly low diffusion rate results in the observations staying more or less in place, while a high diffusion rate makes the observations drift apart, resulting in radical changes to the relationships. With the addition of the diffusion rate, the number of parameters to estimate in terms of the diffusion model is increased by one, to $n \times k + 2$. Note that as $t\rightarrow\infty$, we have $E[Z_t]=Z_0$ regardless of the magnitude of the diffusion rate as $E[\epsilon_t]=0$.
    

\subsection{Autoregressive Model}

    Temporal autoregressive (AR) network models are discussed by \citeauthor{sewell2018simultaneous} in \cite{sewell2018simultaneous}, arguing that the assumption of conditional independence can be quite strong and difficult to justify. Their proposal is a general observation-driven model using a flexible autoregressive approach, extending the diffusion model by allowing predictions to be based on a number of past observations, thus moving away from the standard Markov assumption. 
    
    The autoregressive model considers the change in latent positions to be defined as in \Cref{eq:ar-model}, where $c$ is a constant, $p$ indicates the order of the AR process and the $\phi$s are the coefficients weighing the impact of the previous sets of latent positions.
    
    \begin{equation}\label{eq:ar-model}
        Z_t = c + \sum_{i=1}^p \phi_i Z_{t-i} + \epsilon_t \qquad\qquad \epsilon \sim N(0,\sigma_\epsilon^2)
    \end{equation}
    
    A first-order autoregressive process, denoted AR(1) as $p=1$, with $c=0$ and $\phi_1=1$ is identical to the previously described diffusion process, using only the most recent value in its prediction. Hence, the AR model is capable of capturing diffusion, while extending to periodic behavior as well for higher-order processes ($p>1$). An AR(2) process is able to model a signal which spikes every second time step, while an AR(3) process detects periodicity with an interval of 3, and so on. The order of the process is usually predetermined according to the expected periodic frequency of the data, often referred to as the seasonality. For instance, monthly data has a seasonal interval of twelve as there are twelve months in a year, while daily data has a seasonal interval of seven corresponding to a week.
    
    Increasing the order of the autoregressive model does however also increase model complexity, where an AR(3) process requires three sets of initial latent space positions to produce predictions for time steps $t\geq3$. Hence, the number of parameters to estimate has increased by a three-fold, in addition to extra AR coefficients. For the general AR($p$) model, the number of parameters is $p\times (n\times k + 1) + 2$, requiring an additional set of latent space positions and an extra coefficient for each order of $p$.
    Using all AR coefficients is however not necessary and the basis for predictions can be limited to a fixed set of previous values, also called lags. Considering daily data, an AR(7) process is required to capture the expected weekly seasonality, where the fifth and sixth lags corresponding to the values at $t-5$ and $t-6$, often lack new information. Therefore, a sufficient limitation is to simply model the lags at 1, 2 and 7.
    
    An important concept of time series models such as the AR model, is stationarity, where a stationary process is considered stable. For an AR(1) model, the condition $-1<\phi<1$ is necessary for the process to be stationary. This is proven in \cite{alonso2012autoregressive} by assuming that the process begins with an arbitrary fixed value $Z_0=x$, and writing up subsequent values by successive substitution.
    \begin{equation}
        \begin{split}
            Z_1 & = c + \phi x + \epsilon_1 \\
            Z_2 & = c(1+\phi) + \phi^2x+\phi\epsilon_1 + \epsilon_2 \\
            Z_3 & = c(1+\phi+\phi^2) + \phi^3x + \phi^2\epsilon_1 + \phi\epsilon_2 + \epsilon_3 \\
            & \vdotswithin{=}\qquad\qquad\qquad \vdots \\
            Z_t & = c\sum_{i=0}^{t-1}\phi^i + \phi^tx + \sum_{i=0}^{t-1}\phi^i\epsilon_{t-i}
        \end{split}
    \end{equation}
    The mean, or expected value, of $Z_t$ is shown in \Cref{eq:ar-expectation} where $E[\epsilon_t]=0$.
    \begin{equation}\label{eq:ar-expectation}
        E[Z_t] = c\sum_{i=0}^{t-1}\phi^i+\phi^tx
    \end{equation}
    For the process to be stationary, the mean must remain constant over time, hence both terms must converge to constants. This is the case if $|\phi|<1$, as the first term $\sum_{i=0}^{t-1}\phi^i$ becomes the sum of a geometric progression with ratio $\phi$ and converges to $c/(1-\phi)$, while the second term $\phi^t$ converges to zero. In this case, as $t\rightarrow\infty$, all the variables $Z_t$ have the same expectation $\mu=c/(1-\phi)$.
    
    Other stationarity conditions apply for autoregressive processes of higher order, which are calculated in \cite{giles2012stationarity} for an AR(2) model, resulting in the constraints forming the region in \Cref{eq:ar-2-constraints}.
    \begin{equation}\label{eq:ar-2-constraints}
        -1<\phi_2<1 \qquad \phi_1+\phi_2<1 \qquad \phi_2-\phi_1<1
    \end{equation}
    While these restrictions can be used to control the estimation of the AR parameters, it is not necessarily beneficial to limit the model to only capture stationary processes. Instead, by allowing the autoregressive model to estimate the coefficients freely, the parameters can be used to determine the stationarity of the underlying process.
    
    % Using the lag operator $B$, defined as $BZ_t=Z_{t-1}$, and letting $\tilde{Z}_t=Z_t-\mu$ the general AR(p) process is expressed as 
    % \begin{equation}\label{eq:ar-lag-model}
    %     (1-\phi_1B-...-\phi_pB^p)\tilde{Z}_t=\epsilon_t
    % \end{equation}
    

