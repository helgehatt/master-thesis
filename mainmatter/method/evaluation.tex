\section{Model Evaluation}

The models are evaluated in terms of temporal link prediction, where link data for time steps $t\in [0,T_{split})$ is used to predict the links for $t\in [T_{split},T)$. The first task is to split the dataset into a training and a test set by setting $T_{split}$, where $0 < T_{split} < T$, using roughly the first 80\% of the data for training, and the remainder for model evaluation. Hence, the goal of the model is to predict $T-T_{split}$ time steps ahead. A dataset with $N$ entities consists of an $T\times N\times N$ temporal sociomatrix $\tY$, where $\sub(y)^\t$ is 1 if entities $i$ and $j$ are linked at time $t$, and 0 otherwise. As a result, the task boils down to a binary classification problem, predicting which entities are linked at which time steps.

One metric for evaluating binary classifiers is accuracy, defined as the proportion of true results among the total number of cases examined, shown in \Cref{eq:eval-accuracy} where $TP$, $TN$, $FP$ and $FN$ are True/False Positives/Negatives.

\begin{equation}\label{eq:eval-accuracy}
    Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

Many complex networks are however quite sparse, such that simply predicting the absence of any links would give a fairly high accuracy. As an example, one of the datasets considered has $N=986$ entities with at most 24,929 links at a given time step, resulting in a sparsity of 97.4\%. To account for the class imbalance, a more appropriate measurement is AUC, known as the area under the ROC (Receiver Operating Characteristics) curve. The ROC curve is defined by plotting the true positive rate (TPR) against the false positive rate (FPR) at several classification thresholds, computed as shown in \Cref{eq:eval-tpr-fpr}. By gradually lowering the classification threshold, more observations are classified as positive, thus increasing both the number of False Positives as well as True Positives.

\begin{equation}\label{eq:eval-tpr-fpr}
    TPR = \frac{TP}{TP + FN} \qquad\qquad FPR = \frac{FP}{FP + TN}
\end{equation}

A sample ROC curve is depicted in \Cref{Method/AUC-ROC-Curve} as the solid blue line, with the area under the curve in light blue.
\xfig{\ximg[width=0.7\textwidth]{Method/AUC-ROC-Curve}}
The AUC metric indicates how well a model distinguishes between classes, where an AUC score of 0.5 corresponds to simply making random guesses, illustrated by the dashed line. As the model's AUC score increase, the better it is at predicting 0s as 0s and 1s as 1s, such that an AUC score of 1 corresponds to perfect class separation.

To compute a single AUC score for all $T-T_{split}$ evaluation time steps, the evaluation targets and predictions of sizes $(T-T_{split})\times N\times N$ are concatenated and flattened into two long vectors. Doing so, the targets and predictions are treated independently of their time step, estimating the receiver operating characteristics (ROC) curve collectively.
%The performance of the models are compared for each dataset, computing the receiver operating characteristics (ROC) curve for each model by combining all true labels and predictions in the test set. 
An alternative evaluation method is to compute the AUC score for each time step individually and compare the average AUC across different models. However, doing so discards the relationship between the true positive rate and the false positive rate from which the AUC actually originates.