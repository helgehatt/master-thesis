\chapter{Data}\label{ch:Data}

Datasets are taken from the \citetitle{snapnets}~\cite{snapnets}, containing hundreds of networks from social networks to collaboration networks as well as various web graphs. The size of the networks ranges from hundred nodes to several millions of nodes, with the largest network having almost two billion edges.
Before testing on real-life networks, synthetic datasets are generated based on the developed models to verify that the models are capable of capturing the underlying processes, as well as giving insight to strengths and weaknesses of the different models. 

\section{Synthetic Data}

    Many communication networks such as social graphs usually comprise several groups of people who frequently interact. Hence, the synthetic data is generated by drawing two-dimensional initial positions for the nodes from multivariate Gaussians with known covariance. 
    Each Gaussian simulates a cluster, where the nodes are connected depending on the distance between two nodes and the bias $\beta$. This is formally defined in \Cref{eq:synth-link} where $\sigma(x)$ is the sigmoid function (cf. eq. \ref{eq:lsm-link-sigmoid}) and $\alpha$ is drawn from a uniform distribution to assign links with the probabilities given by the logistic regression.
    \begin{equation}\label{eq:synth-link}
        \sub(y) = 
        \begin{cases}
            1 & \text{if } \sigma(\beta - |z_i - z_j|) \geq \alpha \qquad \alpha\sim U(0,1) \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation}
    The drawn initial positions form the latent space $Z$ at the first time step. Latent space positions for subsequent time steps are generated by making small changes to the latent space. The changes depend entirely on which model the adjustment is supposed to mimic, where a static model assumes no change over time, while a diffusion model adds diffusion at each time step. Once latent space positions have been generated for all time steps, links are drawn according to the link probability, resulting in the target sociomatrix $Y$. To keep the synthetic data simple, only undirected links are considered, as well as limiting the dataset to two dimensions ($k=2$) with 30 entities ($N=30$) over 100 time steps ($T=100$).
    
    \subsection{Static Data}
    
        Initial positions are sampled from three multivariate Gaussians with means $(\pm2,2)$ as well as $(0,-2)$ and covariance $\Sigma=0.5^2\bm{I}$, as illustrated in \Cref{Data/Static/Initial0}. 
        \xfig{\ximg[width=0.7\textwidth]{Data/Static/Initial0}[Latent space positions at time $t=0$]}
        The number of connected nodes depends on the chosen bias, which is used to skew the link probabilities in either direction. \Cref{Data/Static/BiasLow} illustrates the links using a bias of $\beta=-1$ while \Cref{Data/Static/BiasHigh} uses a bias of $\beta=1$, featuring 161 links opposed to 49 with negative bias.
        %The difference is 49 links opposed to 161 using the positive bias, including several links across clusters. 
        \xfig{\xdirow
            {\ximg{Data/Static/BiasLow}[Links using a bias of -1]}
            {\ximg{Data/Static/BiasHigh}[Links using a bias of 1]}
        }
        Real-life networks are usually quite sparse, such that the negative bias of $\beta=-1$ is used unless otherwise is stated, resulting in the number of links shown in \Cref{Data/Static/Links}. Even though the latent space positions remain constant over time, the positions are only used to find the probabilities of links occurring, such that the actual links at each time step are drawn from a uniform distribution according to the link probabilities.
        \xfig{\xdirow
            {\ximg{Data/Static/Links}[Number of links as a function of time]}
            {\xanim{Data/Static/Data}[Animation of the static data]}
        }
        \Cref{Data/Static/Data} shows an animation of the synthetic static dataset for all 100 time steps (click on the image in Adobe Reader to play). The image sequence shows that the latent space positions do not change over time, while the links between nodes appear at random with higher frequency for nodes closer in latent space.
    
    \subsection{Dynamic Data}
    
        The dynamic data is based on the same initial positions as the static data, however assumes that the latent positions for subsequent time steps are equal to the previous with diffusion added (cf. eq. \ref{eq:dsnl-model}). This simulates the effect of observations drifting together and apart, as entities become more and less likely to form connections over time. The diffusion is sampled from a normal distribution with zero mean and $\sigma_\epsilon^2\bm{I}$ variance, known as the diffusion rate. Figures \ref{Data/Dynamic/DiffusionLow} and \ref{Data/Dynamic/DiffusionHigh} illustrates how networks evolve over time with respect to diffusion, adding very little diffusion in the first and a lot in the second network.
        \xfig{\xdirow
            {\xanim{Data/Dynamic/DiffusionLow}[Animation of subsequent time steps adding diffusion with $\sigma_\epsilon=0.01\bm{I}$]}
            {\xanim{Data/Dynamic/DiffusionHigh}[Animation of subsequent time steps adding diffusion with $\sigma_\epsilon=0.25\bm{I}$]}
        }
        The diffusion is not necessarily white noise, such that the direction of the diffusion is modified to simulate the three clusters dividing in half, forming three new clusters.
        The synthetic dynamic dataset is generated using a diffusion rate of $\sigma_\epsilon\bm{I}=0.05$ shown in \Cref{Data/Dynamic/Data}. 
        % making the observations change positions over time, while limiting the drift such that the positions are still possible to predict. 
        \xfig{\xdirow
            {\ximg{Data/Dynamic/Links}[Number of links as a function of time]}
            {\xanim{Data/Dynamic/Data}[Animation of the dynamic data]}
        }
        As the amount of diffusion added in each step increases, the more uncertainty is revolved around the final positions of the observations, making them harder to accurately predict.
    
    \subsection{Periodic Data}
    
        The autoregressive model is able to simulate periodic behavior by defining additional sets of initial positions and using appropriate values for $\phi$. Note that $\phi$ is a $p\times k$-dimensional matrix, while described as a vector of size $p$ when there is no distinction between any of the $k$ latent dimensions.
        The first set of initial positions is the same as the static and dynamic data, while the second and third sets of positions are defined using two Gaussians with means $(\pm2,0)$ and four Gaussians with means $(\pm2,\pm2)$ respectively and a variance of $0.5^2$, illustrated in Figures \ref{Data/Periodic/Initial1} and \ref{Data/Periodic/Initial2}.
        \xfig{\xdirow
            {\ximg{Data/Periodic/Initial1}[Second set of initial positions]}
            {\ximg{Data/Periodic/Initial2}[Third set of initial positions]}
        }
        Having predefined the initial positions for the first three time steps, a periodic pattern emerges using $\phi=[0, 0, 1]$, such that the latent positions for time step $t$ is equal to those at $t-3$, cf. \Cref{eq:ar-model}.
        Hence, the autoregressive model simulates an AR(3) process, including a small amount of diffusion using $\sigma_\epsilon=0.01$, emphasizing the periodicity of the dataset.
        
        The AR-coefficients, $\phi$, have a significant impact on the predictions of the model, where minor changes to the values is the difference between a stationary and non-stationary process. Depending on the underlying process, stationarity is not always desirable, however using too high or too low $phi$s the predictions can quickly converge to infinity or zero over time. 
        To illustrate these two scenarios, \Cref{Data/Periodic/ARNone} shows the positions at the next time step using $\phi=[0,0,0]$ where the observations are separated according to the diffusion rate, being so low that all nodes are connected. 
        \xfig{\xdirow
            {\ximg{Data/Periodic/ARNone}[Positions at $t_3$ using $\phi=[0,0,0]$, resulting in white noise with $\sigma=0.01$.]}
            {\ximg{Data/Periodic/ARAll}[Positions at $t_3$ using $\phi=[1,1,1]$]}
        }
        The other scenario is depicted in \Cref{Data/Periodic/ARAll} using $\phi=[1,1,1]$, where the latent positions at the next time step is a sum of all three most recent sets of positions, resulting in too large distances to barely connect a few nodes.
        
        The generated periodic data has abrupt changes to the number of links over time, as can be expected with the large variations between the sets of initial positions. The changes do however follow a distinct pattern, as shown in \Cref{Data/Periodic/Links}, indicating the periodic behavior animated in \Cref{Data/Periodic/Data}.
        \xfig{\xdirow
            {\ximg{Data/Periodic/Links}[Number of links as a function of time]}
            {\xanim|3|{Data/Periodic/Data}[Animation of the periodic data]}
        }
        
\section{Synthetic AR(2) Data}
    
    The autoregressive model is capable of modeling more than just diffusion and periodicity, exploiting the dependence between previous time steps. By using the predictions for the last two time steps, features such as velocity, acceleration and curvature can be modeled. This is illustrated using new initial positions, shown in \Cref{Data/Velocity/Initial3} for the first time step. 
    \xfig{\ximg[width=0.7\textwidth]{Data/Velocity/Initial3}[Latent space positions at $t_0$ for the synthetic AR(2) data]}
    The displacement of the first set of initial positions to the second set of initial positions is found using $\phi=[1,-1]$, such that $Z_2 = Z_1 - Z_0 + \epsilon$. This idea can be extended to characterize particular patterns as shown in the following datasets. To focus solely on these aspects, the diffusion rate $\sigma_\epsilon$ is set to zero.
        
    \subsection{Velocity Data}
        
        Velocity is captured as constant change in latent positions between time steps in a specific direction. This is generated by defining a second set of initial positions with all observations moved slightly closer to the middle, specifically by $\pm0.05$ in both dimensions. As previously shown, this change is found using $\phi=[1,-1]$, hence velocity is modeled using $\phi=[2,-1]$ to add the change to the most recent set of observations. This corresponds to the autoregressive process shown in \Cref{eq:data-velocity}, omitting $\epsilon$.
        \begin{equation}\label{eq:data-velocity}
            Z_t = 2 Z_{t-1} - Z_{t-2} = Z_{t-1} + (Z_{t-1} - Z_{t-2})
        \end{equation}
        At each time step, the initial change of $\pm0.05$ is added, such that the adjustment remains constant over time, and in the same direction. The number of links over time is shown in \Cref{Data/Velocity/Links} with an animation of the velocity dataset in \Cref{Data/Velocity/Data}.
        \xfig{\xdirow
            {\ximg{Data/Velocity/Links}[Number of links as a function of time]}
            {\xanim{Data/Velocity/Data}[Animation of the velocity data]}
        }
        The number of links increases significantly as the two clusters intertwine, having minimal distance between observations, and decreases as the nodes are separated once again.
        
    \subsection{Acceleration Data}
    
        Acceleration is quite similar to velocity, the only difference being that the change between time steps increases or decreases over time. This is achieved by modifying the values of $\phi$ slightly, such that $\phi=[2+\theta,-1-\theta]$ where the sign of $\theta$ indicates acceleration or deceleration and the magnitude of $\theta$ decides the rate of change. This effect is illustrated in \Cref{DataAccelerationPhi} having an initial change of 1.0 between the first two time steps and generating subsequent values using $\theta=-0.1$ (deceleration), $\theta=0.0$ (constant) and $\theta=0.1$ (acceleration).
        \xtab{\xtex{DataAccelerationPhi}[Example of how acceleration affects changes over time using different values of $\theta$]}
        
        The change between time steps increases or decreases exponentially, such that continuing in this fashion for 100 time steps will quickly result in changes either too large or too small. Hence, the deceleration data shown in \Cref{Data/Acceleration/DataNegative}, is generated using significantly smaller magnitudes, having an initial change of 0.1 between the first two time steps and a minimal $\theta$ of -0.03.
        \xfig{\xdirow
            {\ximg{Data/Acceleration/LinksNegative}[Number of links as a function of time]}
            {\xanim{Data/Acceleration/DataNegative}[Animation showing deceleration]}
        }
        The deceleration data is included for illustrative purposes, whereas the dataset used to assess model behavior is shown in \Cref{Data/Acceleration/DataPositive}, using an initial change of only 0.01 and a positive $\theta$ of $0.03$.
        \xfig{\xdirow
            {\ximg{Data/Acceleration/LinksPositive}[Number of links as a function of time]}
            {\xanim{Data/Acceleration/DataPositive}[Animation of the acceleration data]}
        }
    
    \subsection{Curvature Data}
    
        Having a notion of acceleration, curvature is generated by applying different changes to each of the $k$ latent dimensions. This is done by combining the results of the acceleration and deceleration data, starting with a larger initial change in the $x$-axis and decelerating, while having a smaller initial change in the $y$-axis and accelerating. Over time, the change across the $y$-axis will catch up with the change in $x$-coordinates, thus shifting the direction of the observations. This phenomena is depicted in \Cref{Data/Curvature/Data}, defining the curvature dataset.
        \xfig{\xdirow
            {\ximg{Data/Curvature/Links}[Number of links as a function of time]}
            {\xanim{Data/Curvature/Data}[Animation of the curvature data]}
        }
        Notice the similarity in the number of links over time between acceleration (\cref{Data/Acceleration/LinksPositive}) and curvature (\cref{Data/Curvature/Links}). While the dimensionality of $\phi$ can be used to define complex movements over time, an AR(2) process can just as easily capture the curvature data using a one-dimensional $\phi$, as the model is only concerned with the interactions between entities and not with the true underlying circumstances in which they occur.
        
\section{UC Messaging Data}

    \xfig{UC-Messaging-Data}
    
\section{EU Email Data}

    \xfig{EU-Email-Data}